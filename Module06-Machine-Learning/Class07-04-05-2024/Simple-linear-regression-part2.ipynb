{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Least Error in Simple Linear Regression</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            line-height: 1.6;\n",
    "            margin: 20px;\n",
    "        }\n",
    "        h1 {\n",
    "            font-size: 24px;\n",
    "            margin-bottom: 10px;\n",
    "        }\n",
    "        p {\n",
    "            font-size: 16px;\n",
    "            margin-bottom: 15px;\n",
    "        }\n",
    "        code {\n",
    "            font-family: 'Courier New', Courier, monospace;\n",
    "            font-size: 14px;\n",
    "            background-color: #f4f4f4;\n",
    "            padding: 2px 5px;\n",
    "            border-radius: 3px;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Least Error in Simple Linear Regression</h1>\n",
    "    <p>In simple linear regression, the least error typically refers to minimizing the sum of squared errors (SSE), also known as the residual sum of squares (RSS). This means finding the line that best fits the data by minimizing the squared differences between the observed dependent variable values and the values predicted by the linear model.</p>\n",
    "    <p>Mathematically, if you have a dataset with <code>n</code> data points, and you're fitting a line of the form <code>y = mx + c</code>, where <code>m</code> is the slope and <code>c</code> is the y-intercept, the SSE is calculated as:</p>\n",
    "    <p><code>SSE = ∑(y<sub>i</sub> - (mx<sub>i</sub> + c))<sup>2</sup></code></p>\n",
    "    <p><code>SSE = ∑(y<sub>i</sub> - mx<sub>i</sub> - c)<sup>2</sup></code></p>\n",
    "    <p>The goal is to find the values of <code>m</code> and <code>b</code> that minimize this sum.</p>\n",
    "    <p>One common method to find the least error in simple linear regression is through the ordinary least squares (OLS) method. This method calculates the coefficients (slope and intercept) that minimize the SSE. It does so by taking the partial derivatives of the SSE with respect to <code>m</code> and <code>b</code>, setting them equal to zero, and solving for <code>m</code> and <code>b</code>.</p>\n",
    "    <p>The formulas for the OLS estimates of the slope and intercept are:</p>\n",
    "    <p><code>m = ∑((x<sub>i</sub> - x̄)(y<sub>i</sub> - ȳ)) / ∑((x<sub>i</sub> - x̄)<sup>2</sup>)</code></p>\n",
    "    <p><code>b = ȳ - mx̄</code></p>\n",
    "    <p>Where <code>x̄</code> and <code>ȳ</code> are the means of the independent and dependent variables, respectively.</p>\n",
    "    <p>By minimizing the SSE, you're essentially finding the line that best fits the data, which can be interpreted as the line that minimizes prediction errors.</p>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Gradient Decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent (the negative gradient) as defined by the derivative of the function. It is widely used in machine learning for optimizing the parameters of models.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "Initialize Parameters: Start with initial values for the parameters you want to optimize.\n",
    "\n",
    "Calculate Gradient: Compute the gradient of the function with respect to each parameter. The gradient points in the direction of the greatest rate of increase of the function.\n",
    "\n",
    "Update Parameters: Adjust the parameters in the opposite direction of the gradient to minimize the function. This adjustment is made by taking a step proportional to the negative of the gradient.\n",
    "\n",
    "Repeat: Continue iterating steps 2 and 3 until convergence or until a certain stopping criterion is met (e.g., reaching a maximum number of iterations or the change in parameters becomes very small).\n",
    "\n",
    "The size of the step taken in each iteration is determined by a parameter called the learning rate. Choosing an appropriate learning rate is crucial, as a value that is too small may lead to slow convergence, while a value that is too large may cause the algorithm to overshoot the minimum.\n",
    "\n",
    "There are variations of gradient descent, such as stochastic gradient descent (SGD), mini-batch gradient descent, and more advanced optimization algorithms like Adam, RMSprop, and Adagrad, which adaptively adjust the learning rate and incorporate other enhancements to improve convergence speed and stability.\n",
    "\n",
    "Gradient descent is a fundamental optimization technique used not only in machine learning but also in various other fields like physics, engineering, and economics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Gradient Decent -Closed loop (with equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E = sum(yi-mxi-c)^2\n",
    "\n",
    "Case1:c is zero\n",
    "\n",
    "if c is zero than all the lines which will be drawn will be going only from origin and **if you increase or decrease the **m** the error will increase**\n",
    "and E (error for m equation)\n",
    "\n",
    "E = sum(yi-mxi)^2 as c is zero\n",
    "\n",
    "Case 2: m = 1 slope is constant\n",
    "\n",
    "Error of equation\n",
    "E = sum(yi-xi-c)^2 as m is one\n",
    "\n",
    " So all the lines will be having same slope and only intersect will be changing amd the line will be parallel to each other **if you increase or decrease the **c** the error will increase**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"differentiation-formula-12.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the Slope and intercept in the linear regression model:\n",
    "\n",
    "**What is a slope?**\n",
    "\n",
    "In a regression context, the slope is very important in the equation because it tells you how much you can expect Y to change as X increases.\n",
    "It is denoted by m in the formula y = mx+b.\n",
    "\n",
    "<img src=\"m-slope-formula.webp\">\n",
    "\n",
    "**What is Intercept?**\n",
    "\n",
    "The y-intercept is the place where the regression line y = mx + b crosses the y-axis (where x = 0), and is denoted by b.\n",
    "Formula to calculate the intercept is:\n",
    "\n",
    "<img src=\"intercept-slope-formula.webp\">\n",
    "\n",
    "\n",
    "Now,Put this slope and intercept into the formula (y = mx +b) and their you have the description of the best fit line.\n",
    "\n",
    "\n",
    "<img src=\"best-fit-line.webp\" width=\"450\">\n",
    "\n",
    "This best fit line will now pass through the data according to the properties of a regression line that is discussed below.\n",
    "\n",
    "Now,What if i tell you there is still room to improve the best fit line?\n",
    "As you know, we want our model to be the best performing model on unseen data and to do so Stochastic Gradient Descent is used to update the values of slope and intercept,so that we achieve very low cost function of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    intercept and slope corelation\n",
    "\n",
    "<img src=\"Gradient_Descent_Animation.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative approach for getting the best fit line as using equation will be more time consuming as for each x point doing differential will be difficult as it will consume a lot of compute and time\n",
    "\n",
    "As there could be multiple lines going across which may be required to test for best fit, below are the steps to get the best fit line\n",
    "\n",
    "1. initialize line with a m,c value\n",
    "2. go in the direction to reduce the error\n",
    "3. keep creating a new line until you get the minimal error , or m,c is optimal or cost function is least.\n",
    "\n",
    "in the plot of cost function there will be one point which will be having minimum cost function that is called as global minima\n",
    "\n",
    "<img src =\"global-minima.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Convex function : \n",
    "\n",
    "    when a line will be drawn in convex function it will cut on two points and it will have only one minima which will be global minima\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Non Convex function : \n",
    "\n",
    "    when a line will be drawn in non convex function it will cut on more than two points and it will have local and global minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Saddle point\n",
    "\n",
    "    A saddle point is a critical point of a function where the gradient is zero, but it's neither a local minimum nor a maximum, resembling a saddle shape when visualized in two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"convex-nonconvex.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"convex-nonconvex-1.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Convergence algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple linear regression, convergence typically refers to the process of finding the optimal coefficients (slope and intercept) that minimize the error between the observed and predicted values. This is often achieved using an optimization algorithm such as ordinary least squares (OLS), which iteratively adjusts the coefficients until a convergence criterion is met, such as reaching a minimum error or maximum likelihood estimate.\n",
    "\n",
    "In order to get to the convergence point we need to descent the gradient hence we use gradient decent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"Gradient_Descent.png\" width = \"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient descent** is an optimization algorithm used to minimize a function by iteratively moving in the direction of steepest descent (negative gradient). In the context of linear regression, it's used to minimize the error between observed and predicted values.\n",
    "\n",
    "Here's the basic idea:\n",
    "\n",
    "1. **Initialize Parameters**: Start with initial guesses for the parameters (e.g., slope and intercept).\n",
    "\n",
    "2. **Compute Gradient**: Calculate the gradient of the cost function with respect to each parameter. This tells you the direction and rate of change of the cost function at the current point.\n",
    "\n",
    "3. **Update Parameters**: Adjust the parameters in the direction opposite to the gradient to reduce the cost. This involves subtracting a fraction of the gradient from the current parameter values, scaled by a learning rate (α).\n",
    "\n",
    "4. **Repeat**: Iterate steps 2 and 3 until convergence, which is usually determined by reaching a predefined number of iterations or when the change in parameters becomes very small.\n",
    "\n",
    "By iteratively updating the parameters in the direction that reduces the cost function, gradient descent aims to find the optimal parameter values that minimize the error between observed and predicted values in linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Gradient_Descent-Formula.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"Gradient_Descent-1.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So once the old value is equivalent to new value we need to stop the creation on new line and that point will be the minima**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **learning rate** in gradient descent is a hyperparameter that controls the size of the steps taken during optimization. It determines how much the parameters are adjusted in the direction opposite to the gradient of the cost function. Choosing the right learning rate is crucial for the algorithm's convergence and efficiency.\n",
    "\n",
    "Here are some key points about the learning rate:\n",
    "\n",
    "1. **Effect on Convergence**: A smaller learning rate typically leads to slower convergence but may result in more precise optimization. Conversely, a larger learning rate can speed up convergence but risks overshooting the optimal solution and oscillating around it or diverging.\n",
    "\n",
    "2. **Tuning**: The learning rate is a hyperparameter that needs to be tuned during the training process. This often involves experimentation to find the optimal value that balances convergence speed and accuracy.\n",
    "\n",
    "3. **Gradient Magnitude**: The magnitude of the gradient (the rate of change of the cost function with respect to the parameters) affects the impact of the learning rate. If the gradients are large, a smaller learning rate may be necessary to prevent overshooting.\n",
    "\n",
    "4. **Adaptive Techniques**: In practice, adaptive learning rate techniques such as AdaGrad, RMSprop, and Adam are often used. These methods dynamically adjust the learning rate based on past gradients, which can improve convergence and stability.\n",
    "\n",
    "5. **Regularization**: In some cases, regularization techniques like L1 or L2 regularization can influence the choice of learning rate. Regularization adds penalty terms to the cost function, affecting the gradient and consequently the learning rate's impact on parameter updates.\n",
    "\n",
    "In summary, the learning rate is a crucial parameter in gradient descent that requires careful consideration and tuning to ensure efficient convergence and optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"learning-rate.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"learning-rate.jpg\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://developers.google.com/machine-learning/crash-course/fitter/graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially the jump from old value to be new value will be a large step as the slope will be high and as we decent on the gradient the slope value will be less hence steps will be small\n",
    "\n",
    "<img src=\"gradientstepsize.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
