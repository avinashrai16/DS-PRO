{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN \n",
    "#### (Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm particularly well-suited for identifying clusters of arbitrary shapes and for datasets with noise and outliers.\n",
    "\n",
    "Distance based algorithm like K means and hierarchical are not suited for non linear data spread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN Overview\n",
    "\n",
    "### Key Concepts\n",
    "1. Core Points: Points that have at least a minimum number of neighboring points within a specified radius.\n",
    "2. Border Points: Points that are within the neighborhood of a core point but do not have enough neighbors to be a core point themselves.\n",
    "3. Noise Points: Points that are neither core nor border points; considered outliers.\n",
    "\n",
    "### Parameters\n",
    "* Epsilon (ùúÄ): The maximum distance between two points to be considered as neighbors.\n",
    "* MinPts: The minimum number of points required to form a dense region (including the core point itself).\n",
    "\n",
    "### Algorithm Steps\n",
    "1. Identify Core Points: For each point, calculate the number of points within ùúÄ. If this number is at least MinPts, mark it as a core point.\n",
    "2. Form Clusters: Connect core points that are within Œµ distance of each other to form clusters. Include border points that are within \n",
    "    Œµ distance of a core point.\n",
    "3. Identify Noise: Mark points that are not part of any cluster as noise.\n",
    "\n",
    "### Advantages\n",
    "* Arbitrary Shape Clusters: Can find clusters of any shape, as opposed to K-means which finds spherical clusters.\n",
    "* Noise Handling: Can identify and ignore noise points.\n",
    "* No Need to Specify Number of Clusters: Unlike K-means, does not require the number of clusters to be specified in advance.\n",
    "\n",
    "### Disadvantages\n",
    "* Parameter Sensitivity: Sensitive to the choice of Œµ and MinPts.\n",
    "* Scalability: Can be computationally intensive for large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Feature                     | K-means Clustering                                    | Hierarchical Clustering                               | DBSCAN Clustering                                   |\n",
    "|-----------------------------|-------------------------------------------------------|------------------------------------------------------|----------------------------------------------------|\n",
    "| **Initialization**          | Requires the number of clusters \\( K \\) to be specified beforehand. | Does not require the number of clusters to be specified. | Does not require the number of clusters to be specified. |\n",
    "| **Algorithm Type**          | Partition-based                                      | Hierarchical (can be agglomerative or divisive)      | Density-based                                      |\n",
    "| **Cluster Formation**       | Iteratively updates centroids and assigns points to nearest centroid. | Builds a tree of clusters by successive merging (agglomerative) or splitting (divisive). | Forms clusters based on density of points.         |\n",
    "| **Distance Metric**         | Typically uses Euclidean distance, but others can be used. | Multiple linkage criteria (single, complete, average, Ward's method) with various distance metrics. | Typically uses Euclidean distance, but others can be used. |\n",
    "| **Time Complexity**         | Generally \\( O(n \\cdot k \\cdot t) \\), where \\( n \\) is the number of points, \\( k \\) is the number of clusters, and \\( t \\) is the number of iterations. | Generally \\( O(n^3) \\) for agglomerative methods; more computationally intensive than K-means. | Generally \\( O(n \\log n) \\) for spatial indexing (e.g., using KD-trees). |\n",
    "| **Scalability**             | Scalable to large datasets.                          | Not as scalable to large datasets due to higher computational complexity. | Scalable with appropriate indexing (e.g., KD-trees), but can be slower than K-means. |\n",
    "| **Data Size**               | Suitable for large datasets.                         | More suitable for smaller datasets (typically less than a few thousand data points). | Suitable for large datasets, but performance depends on parameter settings and data distribution. |\n",
    "| **Handling of Data Types**  | Primarily suitable for numerical data. Extensions like K-prototypes exist for mixed data types. | Can handle both numerical and categorical data, depending on the distance metric used. | Primarily suitable for numerical data. Extensions exist for categorical data. |\n",
    "| **Output**                  | Flat partition of clusters.                          | Dendrogram representing nested clusters.             | Flat partition of clusters with noise points identified. |\n",
    "| **Cluster Shape**           | Assumes spherical clusters of similar size.           | Can capture clusters of arbitrary shape and size.    | Can capture clusters of arbitrary shape and size.  |\n",
    "| **Stability**               | Sensitive to initial centroid placement; multiple runs with different initializations recommended. | More stable results as it does not depend on initial parameters. | Stable results, but sensitive to \\(\\varepsilon\\) and MinPts parameters. |\n",
    "| **Handling of Noise**       | Less robust to noise and outliers.                    | More robust to noise and outliers.                   | Specifically designed to handle noise and outliers. |\n",
    "| **Hierarchical Structure**  | Does not provide hierarchical relationships between clusters. | Provides a hierarchy of clusters, useful for exploring data at different levels of granularity. | Does not provide hierarchical relationships between clusters. |\n",
    "| **Memory Usage**            | Typically lower memory usage.                        | Higher memory usage due to the storage of the distance matrix. | Memory usage depends on spatial indexing used and data size. |\n",
    "| **Use Cases**               | Customer segmentation, document clustering, image compression. | Gene expression data analysis, hierarchical document clustering, phylogenetic tree construction. | Anomaly detection, clustering spatial data, noise filtering, geographic data analysis. |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
