{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a technique used in machine learning to prevent over-fitting and improve the generalization performance of a model. It involves adding a penalty term to the loss function during training, which discourages the model from fitting the training data too closely and helps to control the complexity of the model.\n",
    "\n",
    "In other words adding some parameters to the cost function for having some bias added so that model does not over fit the data.\n",
    "\n",
    "If your machine learning model learns the pattern of data very well while training, it can lead to the problem of Overfitting. Overfitting is a situation in which models perform very well on the training data and poor on testing data. To resolve this issue, a penalty term can be added to the equation of the model, this process is called Regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Cost Function of Linear Regression\n",
    "\n",
    "<img src=\"cost-function.webp\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Cost Function of Lasso Regression\n",
    "The cost function of Linear Regression is modified by adding a Regularization term to it.\n",
    "\n",
    "<img src=\"lasso-regression.webp\" width=\"350\">\n",
    "\n",
    "\n",
    "<strong>L = ∑(Ŷi– Yi)² + <span style=\"color: #ff0000;\">λ∑|β|</span></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Cost Function of Ridge Regression\n",
    "The cost function of Linear Regression is modified by adding a Regularization term to it.\n",
    "\n",
    "<img src=\"ridge-regerssion.webp\" width=\"250\">\n",
    "\n",
    "\n",
    "<strong>L = ∑(Ŷi– Yi)² + <span style=\"color: #339966;\">λ∑β²</span></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Elastic net Regression\n",
    "Elastic net is a modified version of linear regression that adds regularization penalties from both lasso and ridge regression during the training.\n",
    "\n",
    "<strong>L = ∑(Ŷi– Yi)² + <span style=\"color: #339966;\">λ∑β²</span> + <span style=\"color: #ff0000;\">λ∑|β|</span></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lasso and Ridge both Cost Function will increase and M/Slope will be decrease but in Lasso the slope can be zero but in Ridge slope will not be zero anytime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Lasso regression** can be used for feature selection while ridge regression \n",
    "doesnot support feature selection.\n",
    "\n",
    "2. **Lasso regression** can be used in cases where less features are significant \n",
    "as it reduces the value of insignificant features to zero whereas **Ridge \n",
    "regression** can be used in cases where more no. of features are significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "<meta charset=\"UTF-8\">\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "<title>Ridge vs Lasso vs Elastic Net Comparison</title>\n",
    "<style>\n",
    "    table {\n",
    "        border-collapse: collapse;\n",
    "        width: 90%;\n",
    "    }\n",
    "    th, td {\n",
    "        border: 1px solid #dddddd;\n",
    "        text-align: left;\n",
    "        padding: 8px;\n",
    "    }\n",
    "    th {\n",
    "        background-color: #f2f2f2;\n",
    "    }\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<h2>Ridge vs Lasso vs Elastic Net Comparison</h2>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Feature</th>\n",
    "        <th>Ridge Regression</th>\n",
    "        <th>Lasso Regression</th>\n",
    "        <th>Elastic Net</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Regularization</td>\n",
    "        <td>L2 regularization</td>\n",
    "        <td>L1 regularization</td>\n",
    "        <td>Combination of L1 and L2 regularization</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Objective Function</td>\n",
    "        <td>Minimize: RSS + α∑βj^2</td>\n",
    "        <td>Minimize: RSS + α∑|βj|</td>\n",
    "        <td>Minimize: RSS + α(λ∑|βj| + (1-λ)∑βj^2)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Usage</td>\n",
    "        <td>Deals with multicollinearity and prevents overfitting</td>\n",
    "        <td>Useful for feature selection by driving some coefficients to zero</td>\n",
    "        <td>Combines benefits of Ridge and Lasso, useful for correlated features</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Shrinking Effect</td>\n",
    "        <td>Shrinks coefficients towards zero</td>\n",
    "        <td>Can zero out coefficients, performing feature selection</td>\n",
    "        <td>Shrinks coefficients towards zero and performs feature selection</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Solution Path</td>\n",
    "        <td>Coefficients tend to be smaller but never exactly zero</td>\n",
    "        <td>Coefficients can be exactly zero, leading to sparse models</td>\n",
    "        <td>Coefficients can be zero and tend to be smaller, balances Ridge and Lasso</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Suitable for</td>\n",
    "        <td>When all features are potentially useful, or when multicollinearity is present</td>\n",
    "        <td>When there are many features and some are irrelevant or redundant</td>\n",
    "        <td>When there are many features, some correlated, and feature selection is needed</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Computational Cost</td>\n",
    "        <td>Less computationally expensive than Lasso</td>\n",
    "        <td>Generally more computationally expensive than Ridge</td>\n",
    "        <td>Moderate computational cost due to combination of L1 and L2 regularization</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    why lasso makes coffeffiecnt 0 and ridge near zero ? find out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
