{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering is a popular and straightforward method used in data mining and statistical data analysis for partitioning a dataset into distinct, non-overlapping groups or clusters. \n",
    "Here's an overview of how K-means clustering works and its key features:\n",
    "\n",
    "#### How K-means Clustering Works\n",
    "1. Initialization:\n",
    "    * Choose the number of clusters, ùêæ.\n",
    "    * Randomly initialize ùêæ centroids (the center points of the clusters).\n",
    "\n",
    "2. Assignment Step:\n",
    "    * Assign each data point to the nearest centroid based on a distance metric (typically Euclidean distance). This forms ùêæ clusters.\n",
    "\n",
    "3.  Update Step:\n",
    "    * Recalculate the centroids as the mean of all data points assigned to each cluster.\n",
    "\n",
    "4. Repeat:\n",
    "    * Repeat the assignment and update steps until the centroids no longer change significantly or a maximum number of iterations is reached.\n",
    "\n",
    "\n",
    "#### Key Features and Considerations\n",
    "*   Number of Clusters (K): The number of clusters,ùêæ, must be specified beforehand. Determining the appropriate K can be done using methods like the elbow method or silhouette analysis.\n",
    "\n",
    "* Distance Metric: K-means typically uses Euclidean distance to assign data points to clusters, but other distance metrics can be used depending on the context.\n",
    "\n",
    "* Scalability: K-means is computationally efficient and can handle large datasets, although it may struggle with very high-dimensional data.\n",
    "\n",
    "* Cluster Shape: K-means assumes clusters to be spherical and of similar size, which may not be suitable for all datasets.\n",
    "\n",
    "* Random Initialization: The algorithm's outcome can depend on the initial placement of centroids, so it is often run multiple times with different initializations, and the best result is chosen.\n",
    "\n",
    "* Speed and Efficiency: K-means is relatively fast and can be optimized further using techniques like the k-means++ initialization method to improve the placement of initial centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"k-means-clustering.png\" width=\"550\" heoght=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"k-means-clustering-1.webp\" width=\"550\" heoght=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance is calculated by Euclidean or Manhattan Distance:\n",
    "\n",
    "<img src=\"manhattan_euclidean-distance.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "### WCSS (With in cluster sum of square distance)\n",
    "##### How to find the optimal value for K?\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"wcss.png\" width=\"680\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many clusters we need in our dataset, maybe it‚Äôs 3, 4 or 10. We need some metric to evaluate, how a certain number of clusters perform and preferably that metric should be quantifiable.\n",
    "\n",
    "Fortunately, there is one metric called Within-Cluster-Sum-Square (WCSS).\n",
    "\n",
    "<img src=\"wcss-formula.webp\">\n",
    "\n",
    "where ‚ÄòY·µ¢‚Äô is centroid for observation ‚ÄòX·µ¢‚Äô and ‚Äôn‚Äô is the total number of observations.\n",
    "\n",
    "***So, from the formula, we can interpret that as the number of clusters increases, the distance between the point and its centroid decreases and hence the WCSS decreases.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, How far it keeps decreasing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let say we have as many clusters as we have data points. In this case, our WCSS will equate to 0 because every single point has its cluster and therefore centroid is exactly where the point is, so the distance between the point and its centroid is 0 and hence WCSS is 0.\n",
    "\n",
    "So, from the above statement, we can interpret that higher the number of clusters lesser is the WCSS value.\n",
    "\n",
    "To find the optimal number of clusters we use the Elbow method which uses the WCSS metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let‚Äôs understand the Elbow method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Elbow method runs K-Means clustering for the dataset for a range of values of ‚ÄòK‚Äô (say 1:10) and for each value of ‚ÄòK‚Äô calculates the WCSS value for all clusters and then plot the graph for different WCSS value.\n",
    "\n",
    "<img src=\"wcss-plot.webp\">\n",
    "\n",
    "And our hint to select optimal is to find the point where the improvement is not great and that point is our elbow point. In the above graph, that point is at 5. So the optimal number of clusters for our example is 5.\n",
    "\n",
    "As we can see that this method is quite arbitrary. Somebody might pick ‚ÄòK‚Äô as 5 or someone else might 4 or 6. This is the judgement call as a data scientist we need to make."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
