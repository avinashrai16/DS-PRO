{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</h2><b>Support Vector Machine(SVM) will be used for classification (SVC) and Regression (SVR)</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/\">SMV Introduction (Web Link)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a supervised machine learning problem where we try to find a hyperplane that best separates the two classes. \n",
    "**Note**: Don‚Äôt get confused between SVM and logistic regression. Both the algorithms try to find the best hyperplane, but the main difference is logistic regression is a probabilistic approach whereas support vector machine is based on statistical approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the question is which hyperplane does it select? There can be an infinite number of hyperplanes passing through a point and classifying the two classes perfectly. So, which one is the best?\n",
    "\n",
    "Well, SVM does this by finding the maximum margin between the hyperplanes that means maximum distances between the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Important Terms</h1>\n",
    "Now let‚Äôs define two main terms which will be repeated again and again in this article:\n",
    "\n",
    "* *<u>Support Vectors</u>*: These are the points that are closest to the hyperplane. A separating line will be defined with the help of these data points.\n",
    "\n",
    "* *<u>Margin*</u>: it is the distance between the hyperplane and the observations closest to the hyperplane (support vectors). In SVM large margin is considered a good margin. There are two types of margins **hard margin** and **soft margin**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"SVM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum 2 support vectors are required for SVC (Support vector classifier). SVC is also called as Margin classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Since we are plotting the data points in a 2-dimensional graph we call this decision boundary a straight line but if we have more dimensions, we call this decision boundary a ‚Äúhyperplane‚Äù***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    **Hard Margin SVM** vs **Soft Margin SVM**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard margin and soft margin are concepts from Support Vector Machines (SVM), a popular machine learning algorithm for classification tasks. These concepts relate to how the SVM handles data points relative to the decision boundary, particularly when dealing with non-linearly separable data. Let's explore both concepts and illustrate them with charts.\n",
    "\n",
    "***Hard Margin SVM***\n",
    "* Definition: The hard margin SVM assumes that the data is linearly separable and tries to find the maximum margin hyperplane that perfectly separates the data points of different classes.\n",
    "* Constraints: No data points are allowed to be on the wrong side of the margin or within the margin.\n",
    "\n",
    "***Soft Margin SVM***\n",
    "* Definition: The soft margin SVM allows some misclassification and margin violations to handle non-linearly separable data. It introduces a penalty for misclassified points and margin violations.\n",
    "* Constraints: Some data points are allowed to be on the wrong side of the margin or within the margin, controlled by a regularization parameter ùê∂ (no. of data points which can be sacrificed for allowing the misclassification for better accuracy as most of the data in real life has some kind of overlapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the details of hard margin and soft margin SVMs:\n",
    "\n",
    "| **Aspect**               | **Hard Margin SVM**                               | **Soft Margin SVM**                               |\n",
    "|--------------------------|---------------------------------------------------|---------------------------------------------------|\n",
    "| **Assumption**           | Data is linearly separable                        | Data may not be linearly separable                |\n",
    "| **Decision Boundary**    | Finds the maximum margin hyperplane               | Finds a hyperplane with a trade-off between margin size and classification error |\n",
    "| **Margin**               | No points allowed within the margin               | Allows some points within the margin              |\n",
    "| **Misclassification**    | No misclassification is allowed                   | Allows some misclassifications                    |\n",
    "| **Support Vectors**      | Only points exactly on the margin                 | Points within the margin and on the margin        |\n",
    "| **Regularization Parameter \\(C\\)** | Implicitly very large (practically infinite) | Tunable parameter that controls the trade-off     |\n",
    "| **Use Case**             | Perfectly separable data                          | Non-separable or noisy data                       |\n",
    "| **Flexibility**          | Less flexible (strict separation)                 | More flexible (tolerates violations)              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"soft-vs-hard margin-svm.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the plots illustrating the decision boundaries and margins for both hard margin and soft margin SVMs:\n",
    "\n",
    "***Hard Margin SVM:***\n",
    "\n",
    "* The decision boundary perfectly separates the two classes without any points within the margin or misclassified.\n",
    "* The support vectors lie exactly on the margin.\n",
    "\n",
    "***Soft Margin SVM:***\n",
    "\n",
    "* The decision boundary allows for some points within the margin or misclassified.\n",
    "* The support vectors include points that lie within the margin or on the wrong side of the decision boundary.\n",
    "* These visualizations help to understand how SVMs can adapt to different data distributions and the role of the margin in classification. The soft margin SVM provides flexibility to handle non-linearly separable data by introducing a trade-off between margin size and classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For mathematical intuition refer :\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"SVM-CostFunction-Max.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that max[f(x)] can also be written as min[1/f(x)], it is common practice to minimize a cost function for optimization problems; therefore, we can invert the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"SVM-CostFunction-Min.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***These above equations are for Hard Margin,To make a soft margin equation we add 2 more terms to this equation which is zeta and multiply that by a hyperparameter ‚Äòc‚Äô***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"SVM-SoftMargi-Cost-Fun.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the correctly classified points our zeta will be equal to 0 and for all the incorrectly classified points the zeta is simply the distance of that particular point from its correct hyperplane that means if we see the wrongly classified green points the value of zeta will be the distance of these points from L1 hyperplane and for wrongly classified redpoint zeta will be the distance of that point from L2 hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"SVM-Error.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can say that our that are **SVM Error = Margin Error + Classification Error**. The higher the margin, the lower would-be margin error, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*When a miss classification occurs, it is because a given point is on the wrong side of the separating hyperplane, and that's called a **classification error**. Whenever a point is inside the margin, that counts as a **margin error**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs say you take a high value of ‚Äòc‚Äô =1000, this would mean that you don‚Äôt want to focus on margin error and just want a model which doesn‚Äôt misclassify any data point.\n",
    "\n",
    "Look at the figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"SVM-Error1.png\" width=\"550\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
