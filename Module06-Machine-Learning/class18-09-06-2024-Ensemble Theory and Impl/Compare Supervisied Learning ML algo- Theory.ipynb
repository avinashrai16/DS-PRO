{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Here is the updated chart with algorithms listed from low to high complexity:\n",
    "\n",
    "| Algorithm                      | Type                | When to Use                                                                                 | Pros                                                                                          | Cons                                                                                       |\n",
    "|--------------------------------|---------------------|---------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|\n",
    "| **Linear Regression**          | Regression          | Simple, linear relationships                                                                 | Simple to implement and interpret                                                              | Assumes linearity, sensitive to outliers, poor for non-linear relationships                |\n",
    "| **Logistic Regression**        | Classification      | Simple, binary classification, interpretability important                                   | Easy to implement, interpret, computationally efficient                                       | Assumes linear relationship, not suitable for complex relationships                       |\n",
    "| **K-Nearest Neighbors (KNN)**  | Classification      | Small datasets, low dimensionality, non-linear decision boundaries                           | Simple, no assumptions about data distribution                                                 | Computationally expensive, sensitive to noise and irrelevant features                     |\n",
    "| **Ridge/Lasso Regression**     | Regression          | Linear relationships, high-dimensional data with multicollinearity                           | Reduces overfitting, handles multicollinearity, Lasso for feature selection                    | Assumes linearity, requires tuning of regularization parameter                            |\n",
    "| **Decision Trees**             | Classification/Regression | Interpretability important, handles both numerical and categorical data                       | Easy to interpret, visualize, handles mixed data types                                         | Prone to overfitting, sensitive to noisy data                                              |\n",
    "| **Polynomial Regression**      | Regression          | Non-linear relationships that can be approximated by polynomial functions                    | Captures non-linear relationships                                                              | Prone to overfitting, higher complexity                                                    |\n",
    "| **Support Vector Machines (SVM)** | Classification      | Binary/multi-class, high-dimensional spaces, non-linear boundaries                           | Effective in high-dimensional spaces, versatile with kernels                                   | Memory-intensive, challenging to interpret, requires careful parameter tuning             |\n",
    "| **Support Vector Regression (SVR)** | Regression          | Non-linear relationships, high-dimensional spaces                                            | Effective in high-dimensional spaces                                                           | Memory-intensive, requires careful parameter tuning                                        |\n",
    "| **Random Forest**              | Classification/Regression | Large datasets, need robustness and accuracy, reduce overfitting compared to single trees    | Reduces overfitting, handles missing values well, scalable                                     | Less interpretable than single trees, computationally intensive                           |\n",
    "| **Random Forest Regression**   | Regression          | Large datasets, need robustness and accuracy                                                 | Reduces overfitting, handles missing values well, scalable                                     | Less interpretable, computationally intensive                                              |\n",
    "| **Gradient Boosting (GBM/XGBoost)** | Classification/Regression | High prediction accuracy, handling complex relationships                                     | High accuracy, handles missing data well                                                       | Prone to overfitting, longer training times, complex parameter tuning                     |\n",
    "| **Gradient Boosting Regression (GBM/XGBoost)** | Regression          | High prediction accuracy, handling complex relationships                                     | High accuracy, handles missing data well                                                       | Prone to overfitting, longer training times, complex parameter tuning                     |\n",
    "| **Neural Networks**            | Classification/Regression | Large datasets, complex patterns, high-dimensional data, images, or text                     | Captures complex relationships, highly flexible                                                | Requires large data, computationally expensive, difficult to interpret, needs tuning      |\n",
    "| **Neural Networks**            | Regression          | Large datasets, complex patterns, high-dimensional data, time-series prediction              | Captures complex relationships, highly flexible                                                | Requires large data, computationally expensive, difficult to interpret, needs tuning      |\n",
    "\n",
    "This version of the chart orders the algorithms from lower to higher complexity within the context of supervised learning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
