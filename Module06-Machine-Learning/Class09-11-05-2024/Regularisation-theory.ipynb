{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a technique used in machine learning to prevent over-fitting and improve the generalization performance of a model. It involves adding a penalty term to the loss function during training, which discourages the model from fitting the training data too closely and helps to control the complexity of the model.\n",
    "\n",
    "There are two main types of regularization commonly used in machine learning:\n",
    "\n",
    "1. **L1 Regularization (Lasso Regression)**:\n",
    "   - L1 regularization adds a penalty term to the loss function that is proportional to the absolute value of the coefficients of the model.\n",
    "   - It encourages sparsity in the model by driving some of the coefficients to exactly zero, effectively performing feature selection.\n",
    "   - L1 regularization is particularly useful when dealing with high-dimensional data where feature selection is important.\n",
    "\n",
    "2. **L2 Regularization (Ridge Regression)**:\n",
    "   - L2 regularization adds a penalty term to the loss function that is proportional to the square of the coefficients of the model.\n",
    "   - It penalizes large coefficients but does not generally force them to zero, leading to more stable and numerically well-conditioned solutions compared to L1 regularization.\n",
    "   - L2 regularization is effective in reducing overfitting and improving the generalization performance of the model.\n",
    "\n",
    "The choice between L1 and L2 regularization depends on the specific problem and the characteristics of the data. In some cases, a combination of both L1 and L2 regularization, known as Elastic Net regularization, may be used to take advantage of the benefits of both techniques.\n",
    "\n",
    "Regularization parameters, such as the regularization strength (lambda or alpha), need to be tuned to find the optimal balance between fitting the training data well and preventing overfitting. Techniques like cross-validation can be used to select the best regularization parameters for a given model.\n",
    "\n",
    "Overall, regularization is a powerful tool in machine learning for controlling model complexity and improving the robustness and generalization performance of models, especially in situations with limited training data or high-dimensional feature spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Lasso regression\n",
    "\n",
    "<img src=\"Lasso Regression formula.png\" width=\"600\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Ridge Regression\n",
    "\n",
    "<img src=\"ridge formula-part1.png\" width=\"650\">\n",
    "\n",
    "\n",
    "Similar to lasso regression, the purpose of the regularization term in ridge regression is to shrink the coefficients towards zero. However, ridge regression penalizes the squared values of the coefficients, which tends to shrink the coefficients more evenly compared to lasso regression. Ridge regression does not perform variable selection but instead reduces the magnitude of all coefficients simultaneously.\n",
    "\n",
    "The optimization problem is typically solved using techniques like gradient descent \n",
    "or closed-form solutions (e.g., using the normal equation) to find the values of the coefficients (ùõΩùëó) that minimize the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Elastic Regression\n",
    "\n",
    "<img src=\"elastic net regression.png\" width=\"700\">\n",
    "\n",
    "\n",
    "Elastic Net regression combines the feature selection capability of Lasso regression with the regularization and coefficient shrinkage properties of Ridge regression. This can be particularly useful when dealing with datasets with high dimensionality and collinear features, where Lasso regression may select only one of the correlated features, while Elastic Net can select groups of correlated features.\n",
    "\n",
    "The optimal values of ùúÜ1  and ùúÜ2\n",
    "‚Äãare typically determined through techniques such as cross-validation. Elastic Net regression is commonly used in situations where there is a high degree of multicollinearity among the independent variables or when feature selection is desired along with regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
