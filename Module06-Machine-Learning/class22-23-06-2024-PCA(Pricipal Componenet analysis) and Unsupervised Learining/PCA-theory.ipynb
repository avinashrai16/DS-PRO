{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a widely used technique in machine learning for dimensionality reduction. It transforms the data into a new coordinate system such that the greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on. Here’s a detailed explanation of PCA:\n",
    "\n",
    "***Why PCA?***\n",
    "\n",
    "1. Dimensionality Reduction: To reduce the number of variables (features) in the data while retaining most of the variability (information).\n",
    "2. Visualization: To visualize high-dimensional data in 2D or 3D plots.\n",
    "3. Noise Reduction: To remove noise and redundant features.\n",
    "4. Computational Efficiency: To speed up machine learning algorithms by reducing the number of features.\n",
    "\n",
    "***Steps to Perform PCA***\n",
    "1. Standardize the Data: PCA is affected by the scale of the variables, so it’s important to standardize the data to have a mean of zero and a variance of one.\n",
    "\n",
    "2. Compute the Covariance Matrix: The covariance matrix represents the covariance (a measure of how much two random variables vary together) between each pair of the variables in the data.\n",
    "\n",
    "3. Compute the Eigenvalues and Eigenvectors: The eigenvalues and eigenvectors of the covariance matrix provide the principal components. The eigenvectors represent the direction of the maximum variance, and the eigenvalues represent the magnitude of this variance in the direction of the eigenvector.\n",
    "\n",
    "4. Sort Eigenvalues and Eigenvectors: Sort the eigenvalues in descending order and rearrange the eigenvectors to match this order. The top k eigenvectors (where k is the number of dimensions you want to keep) are the principal components.\n",
    "\n",
    "5. Transform the Data: Project the original data onto the new set of principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Advantage for Principal Component Analysis***\n",
    "1. Used for Dimensionality Reduction\n",
    "2. PCA will assist you in eliminating all related features, sometimes referred to as multi-collinearity.\n",
    "3. The time required to train your model is now substantially shorter because to PCA’s reduction in the number of features.\n",
    "4. PCA aids in overcoming overfitting by eliminating the extraneous features from your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Disadvantage for Principal Component Analysis***\n",
    "1. Useful for quantitative data but not effective with qualitative data.\n",
    "2. Interpretation of PC is difficult from original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Application for Principal Component Analysis***\n",
    "* Computer Vision\n",
    "* Bio-informatics application\n",
    "* For compressed images or resizing of the image\n",
    "* Discovering patterns from high-dimensional data\n",
    "* Reduction of dimensions\n",
    "* Multidimensional Data – Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The vectors which only changes the magnitude not the direction while using covariance matrix is call Eigen Vector. And the number of such vectors will eb equals to dimensions of matrix / no of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to calculate Eigen Value and Vectors\n",
    "\n",
    "update the notes from class PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
