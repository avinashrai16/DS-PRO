{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <b><u> Unsupervised learning:</b></u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning is a type of machine learning where the algorithm is trained on data without labeled responses. The goal is to uncover hidden patterns or intrinsic structures from input data. Unsupervised learning is particularly useful in situations where the data is unlabeled, making it difficult to apply supervised learning techniques. Here are some key concepts, techniques, and applications of unsupervised learning in machine learning:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Clustering**: Grouping a set of objects in such a way that objects in the same group (or cluster) are more similar to each other than to those in other groups. Common clustering algorithms include:\n",
    "   - **K-means Clustering**: Partitions the data into \\( K \\) clusters by minimizing the variance within each cluster.\n",
    "   - **Hierarchical Clustering**: Builds a hierarchy of clusters either by a bottom-up (agglomerative) or top-down (divisive) approach.\n",
    "   - **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: Forms clusters based on the density of points in the data space, capable of finding arbitrarily shaped clusters and handling noise.\n",
    "\n",
    "2. **Dimensionality Reduction**: Reducing the number of random variables under consideration by obtaining a set of principal variables. Techniques include:\n",
    "   - **Principal Component Analysis (PCA)**: Projects the data onto a lower-dimensional space by maximizing the variance.\n",
    "   - **t-Distributed Stochastic Neighbor Embedding (t-SNE)**: Reduces dimensionality while preserving the local structure of data, useful for visualization in 2D or 3D.\n",
    "   - **Autoencoders**: Neural networks used for learning efficient codings of input data by training the network to ignore noise and reduce dimensionality.\n",
    "\n",
    "3. **Association Rule Learning**: Identifying interesting relations between variables in large databases. Techniques include:\n",
    "   - **Apriori Algorithm**: Identifies frequent itemsets and constructs association rules from them.\n",
    "   - **Eclat Algorithm**: Uses a depth-first search approach to find frequent itemsets.\n",
    "\n",
    "4. **Anomaly Detection**: Identifying rare items, events, or observations which raise suspicions by differing significantly from the majority of the data. Techniques include:\n",
    "   - **Isolation Forest**: Identifies anomalies by isolating observations.\n",
    "   - **One-Class SVM**: Identifies the boundary of the normal data points in the feature space and classifies points outside this boundary as anomalies.\n",
    "\n",
    "### Techniques\n",
    "\n",
    "1. **K-means Clustering**:\n",
    "   - Initialize \\( K \\) centroids randomly.\n",
    "   - Assign each data point to the nearest centroid.\n",
    "   - Recalculate the centroids as the mean of all data points assigned to each centroid.\n",
    "   - Repeat the assignment and update steps until convergence.\n",
    "\n",
    "2. **Hierarchical Clustering**:\n",
    "   - Agglomerative: Start with each data point as a single cluster and iteratively merge the closest clusters.\n",
    "   - Divisive: Start with all data points in one cluster and iteratively split the cluster into smaller clusters.\n",
    "\n",
    "3. **Principal Component Analysis (PCA)**:\n",
    "   - Standardize the data.\n",
    "   - Compute the covariance matrix.\n",
    "   - Perform eigenvalue decomposition to obtain eigenvectors (principal components).\n",
    "   - Project the data onto the selected principal components.\n",
    "\n",
    "4. **t-SNE**:\n",
    "   - Compute pairwise similarities in the high-dimensional space.\n",
    "   - Compute pairwise similarities in the low-dimensional space.\n",
    "   - Minimize the divergence between these two similarity distributions using gradient descent.\n",
    "\n",
    "5. **Autoencoders**:\n",
    "   - Train a neural network to map the input to a lower-dimensional latent space (encoding).\n",
    "   - Reconstruct the input from the latent space (decoding).\n",
    "   - Minimize the reconstruction error.\n",
    "\n",
    "### Applications\n",
    "\n",
    "1. **Customer Segmentation**: Grouping customers based on purchasing behavior for targeted marketing.\n",
    "2. **Image Compression**: Reducing the size of image files while maintaining quality using techniques like PCA and autoencoders.\n",
    "3. **Anomaly Detection**: Identifying fraudulent transactions, network intrusions, or rare diseases.\n",
    "4. **Market Basket Analysis**: Discovering associations between products in transaction data for cross-selling strategies.\n",
    "5. **Document Clustering**: Organizing large sets of documents into meaningful clusters for easier retrieval and summarization.\n",
    "6. **DBSCAN** :DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a popular clustering algorithm used in machine learning and data mining. It is designed to identify clusters of varying shapes and sizes in data that contain noise and outliers. Unlike other clustering methods such as k-means, DBSCAN does not require specifying the number of clusters in advance. Here is a detailed overview of DBSCAN, including its principles, algorithm, parameters, advantages, and limitations.\n",
    "\n",
    "### Tools and Libraries\n",
    "\n",
    "- **Python**: Libraries like scikit-learn, TensorFlow, Keras, and PyTorch provide implementations of unsupervised learning algorithms.\n",
    "- **R**: Packages such as `cluster`, `factoextra`, and `arules`.\n",
    "- **MATLAB**: Functions and toolboxes for clustering, dimensionality reduction, and other unsupervised techniques.\n",
    "\n",
    "Unsupervised learning is essential for exploratory data analysis, pattern recognition, and knowledge discovery in datasets without predefined labels. By leveraging these techniques, you can gain insights into the underlying structure of your data and make informed decisions based on these insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20231124111325/Unsupervised-learning.png\" width=\"275\" height=\"200\">\n",
    "<img src=\"https://www.bombaysoftwares.com/_next/image?url=https%3A%2F%2Fbs-cms-media-prod.s3.ap-south-1.amazonaws.com%2FUnsupervised_Learning_951a02cb59.jpg&w=3840&q=75\" width=\"275\" height=\"200\">\n",
    "<img src=\"https://cdn-images-1.medium.com/v2/resize:fit:1600/1*C7meM0f0_oy8RPM9HDb5gg.png\" width=\"275\" height=\"200\">\n",
    "<img src=\"https://eastgate-software.com/wp-content/uploads/2023/10/Unsupervised-Learning-Clustering.png\" width=\"275\" height=\"200\">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
