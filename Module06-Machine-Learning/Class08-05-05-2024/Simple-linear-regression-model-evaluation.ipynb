{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Model Evaluation (Performance of model)\n",
    "        R Square\n",
    "        adjusted R Square\n",
    "        MSE\n",
    "        RMSE\n",
    "        MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decompose variability into the **sum of squares total (SST)**, the **sum of squares regression (SSR)**, and the **sum of squares error (SSE)**. The decomposition of variability helps us understand the sources of variation in our data, assess a model’s goodness of fit, and understand the relationship between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"SSE-SST-SSR.png\" width = \"300\">\n",
    "<img src=\"SSE-SST-SSR-Formula.png\" width=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    R-squared (R²)\n",
    "\n",
    "is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable(s) in a regression model. It's a goodness-of-fit measure for linear regression models.\n",
    "\n",
    "In simple terms:\n",
    "\n",
    "R² value ranges from 0 to 1.\n",
    "\n",
    "0 indicates that the model explains none of the variability of the response data around its mean.\n",
    "\n",
    "1 indicates that the model explains all the variability of the response data around its mean.\n",
    "\n",
    "Values between 0 and 1 represent the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n",
    "\n",
    "<img src=\"R-Squared-1.png\" width=\"150\">\n",
    "\n",
    "<img src=\"R-Squared.jpg\" width=\"150\">\n",
    "\n",
    "\n",
    "R² can be interpreted as the percentage of the response variable variation that is explained by a linear model. For example, an R² of 0.80 means that 80% of the variance in the dependent variable can be explained by the independent variable(s) in the model.\n",
    "\n",
    "In practice, R² is a useful metric to evaluate the goodness-of-fit of a regression model. However, it should be used in conjunction with other metrics and visualizations to fully understand the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Adjusted R-squared (R² adj) \n",
    "    \n",
    "is a modified version of the R-squared (R²) value that adjusts for the number of predictors (independent variables) in a regression model. While R² increases as more predictors are added to the model, it doesn't necessarily mean that the model is improving. The adjusted R-squared value corrects this by penalizing the addition of unnecessary predictors that don't improve the model's performance.\n",
    "\n",
    "Mathematically, adjusted R-squared is calculated as:\n",
    "\n",
    "<img src =\"Adjusted-R-Squared.jpg\" width=\"480\">\n",
    "\n",
    "    \n",
    "Adjusted R-squared accounts for the number of predictors in the model and penalizes the addition of unnecessary predictors. It tends to be lower than the ordinary R-squared value when additional predictors don't improve the model's fit.\n",
    "\n",
    "In summary, adjusted R-squared is a useful metric for evaluating the goodness-of-fit of a regression model while considering the number of predictors included in the model. It provides a more conservative estimate of the model's explanatory power, taking into account the potential over fitting caused by adding unnecessary predictors.\n",
    "\n",
    "\n",
    "**Only add feature in the model if the difference b/w R square and adjusted R square is not more than 3 to 5 %**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    MSE-Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE stands for Mean Squared Error. It is a common metric used to evaluate the performance of a regression model. The MSE measures the average of the squares of the errors or residuals—that is, the difference between the predicted values and the actual values.\n",
    "\n",
    "Mathematically, the MSE is calculated as:\n",
    "\n",
    "<img src=\"mse.png\" width=\"280\">\n",
    "\n",
    "The MSE quantifies the average squared difference between the predicted and actual values. It's important to note that MSE penalizes large errors more than smaller errors due to squaring each error term. Lower values of MSE indicate better model performance, as they suggest that the model's predictions are closer to the actual values.\n",
    "\n",
    "MSE is widely used because it provides a single, easy-to-understand measurement for the overall model performance. However, it does not have the same intuitive interpretation as metrics like R-squared, which measures the proportion of variance explained by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    MAE-Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE stands for Mean Absolute Error. Like MSE, it's a common metric used to evaluate the performance of a regression model. However, instead of squaring the differences between predicted and actual values (as done in MSE), MAE computes the average of the absolute differences between predicted and actual values.\n",
    "\n",
    "Mathematically, the MAE is calculated as:\n",
    "\n",
    "<img src=\"mae.png\" width=\"250\">\n",
    "\n",
    "MAE provides a measure of the average magnitude of the errors between predicted and actual values. Unlike MSE, which penalizes larger errors more heavily due to squaring, MAE treats all errors equally. This makes MAE more robust to outliers compared to MSE.\n",
    "\n",
    "**Interpreting MAE is straightforward: lower values indicate better model performance**, as they suggest that the model's predictions are closer to the actual values. MAE is often preferred when the presence of outliers is a concern, as it provides a more balanced view of the model's accuracy across the entire range of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    RMSE-Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE stands for Root Mean Squared Error. It's another commonly used metric for evaluating the performance of a regression model, particularly when you want to interpret errors in the same units as the target variable.\n",
    "\n",
    "RMSE is simply the square root of the Mean Squared Error (MSE)\n",
    "\n",
    "<img src =\"rmse.png\" width=\"250\"> \n",
    "\n",
    "RMSE shares the same advantages and disadvantages as MSE, but it's more interpretable because it's in the same units as the target variable. Lower values of RMSE indicate better model performance, just like with MSE and MAE.\n",
    "\n",
    "RMSE is often preferred in situations where you want to understand the magnitude of the errors in the context of the original scale of the data. For example, if you're predicting house prices, a RMSE of $10,000 might be easier to interpret than a MSE of 100 million."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
