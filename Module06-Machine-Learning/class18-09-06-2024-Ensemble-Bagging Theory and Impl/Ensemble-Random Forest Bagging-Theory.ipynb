{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Random Forest Classification/Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble learning method that constructs multiple decision trees during training. ***Each tree is trained on a bootstrap sample of the data and a random subset of features.*** Predictions are made by aggregating the predictions of all trees (averaging for regression, voting for classification). It reduces overfitting, handles high-dimensional data well, and is robust to noise and outliers, making it a versatile choice for predictive modeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple tress are created hence Forest and the feature and data selection is random hence this algorithm is called Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"random forest.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap sampling is a statistical technique where samples are drawn with replacement from a dataset to create multiple subsets of the same size. Each subset may contain duplicate instances and miss some original ones. This method helps estimate the variability of a statistic or model performance without assuming a specific distribution of the data. It's commonly used in resampling methods like bootstrapping to assess the uncertainty and stability of statistical estimates or machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sampling: With respect to statistics, sampling is the process of selecting a subset of items from a vast collection of items (population) to estimate a certain characteristic of the entire population\n",
    "\n",
    "* Sampling with replacement: It means a data point in a drawn sample can reappear in future drawn samples as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bootstrap-sampling - Example1:\n",
    "\n",
    "<img src=\"bootstrap-sampling1.webp\" width=\"550\">\n",
    "\n",
    "bootstrap-sampling - Example2:\n",
    "\n",
    "<img src=\"bootstrap-sampling2.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random forest reduces variance how ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest reduces variance primarily through two mechanisms:\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating)**: Random Forest employs bagging, where multiple decision trees are trained on different bootstrap samples (random samples with replacement) from the original dataset. Each tree learns different aspects of the data because of these variations in training data. By averaging (or voting) over multiple trees, Random Forest reduces the variance of the model compared to a single decision tree. This averaging process smooths out individual tree predictions, leading to a more stable and less overfitted model.\n",
    "\n",
    "2. **Random Feature Selection**: In addition to training each tree on a bootstrap sample, Random Forest further reduces variance by randomly selecting a subset of features at each split point in each decision tree. This process introduces additional diversity among the trees because different subsets of features are considered for each split. As a result, the ensemble model captures a wider range of features and reduces the correlation between individual trees, further enhancing the model's generalization ability.\n",
    "\n",
    "### Benefits of Variance Reduction in Random Forest:\n",
    "\n",
    "- **Improved Generalization**: By reducing variance, Random Forest improves the model's ability to generalize to new, unseen data. It minimizes the risk of overfitting, where the model learns noise and specific patterns of the training data that do not generalize well.\n",
    "\n",
    "- **Robustness to Noise**: Random Forest is less sensitive to noise and outliers compared to individual decision trees because noise tends to average out across multiple trees in the ensemble.\n",
    "\n",
    "- **Handling High-Dimensional Data**: Random Forest can effectively handle datasets with a large number of features (high-dimensional data) without significant overfitting, making it suitable for a wide range of real-world applications.\n",
    "\n",
    "In summary, Random Forest reduces variance by aggregating predictions from multiple decision trees trained on different subsets of data and features. This ensemble approach enhances model stability, robustness, and generalization performance compared to individual decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOB (Out-of-Bag) score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OOB (Out-of-Bag) score is a metric used in ensemble methods, particularly in bagging techniques like Random Forests. Here's a concise explanation of the OOB score:\n",
    "\n",
    "### OOB Score in Random Forest:\n",
    "\n",
    "- **Definition**: In Random Forest, each decision tree is trained on a bootstrap sample of the original dataset. This means that some instances are not included (out-of-bag) in the training of each individual tree.\n",
    "  \n",
    "- **Usage**: The OOB score measures the prediction accuracy of each data point using only the trees that do not contain that point in their bootstrap sample. Essentially, for each instance \\( i \\) in the original dataset, the OOB score is computed by aggregating predictions from all trees in the forest that were not trained using \\( i \\).\n",
    "  \n",
    "- **Advantages**:\n",
    "  - It provides an unbiased estimate of the model's performance on new, unseen data because each instance serves as a test case exactly once across all trees.\n",
    "  - Helps to assess the model's generalization ability without needing a separate validation set or cross-validation.\n",
    "\n",
    "- **Calculation**: \n",
    "  - For each instance \\( i \\):\n",
    "    1. Collect predictions from all trees where \\( i \\) was not included in their bootstrap sample.\n",
    "    2. Aggregate these predictions (e.g., by averaging for regression, voting for classification).\n",
    "    3. Compare the aggregated prediction to the true label or value to compute accuracy or error metrics.\n",
    "\n",
    "### Benefits of OOB Score:\n",
    "\n",
    "- **Efficiency**: Eliminates the need for a separate validation set or cross-validation, saving computation time.\n",
    "- **Unbiased Estimate**: Provides a reliable estimate of the model's performance on new data, considering all instances in the original dataset.\n",
    "\n",
    "### Example Use Case:\n",
    "\n",
    "Suppose you have a dataset with 1000 instances. In Random Forest training:\n",
    "- Each tree is trained on a bootstrap sample, typically around 63% of the original dataset (due to sampling with replacement).\n",
    "- The remaining 37% of instances (out-of-bag) are used to calculate the OOB score for each instance, by aggregating predictions from trees that did not include that instance in their training set.\n",
    "\n",
    "In summary, the OOB score in Random Forest is a valuable metric that leverages the ensemble's structure to provide an efficient and unbiased estimate of model performance, particularly useful for assessing generalization and avoiding overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOB score is computed as the number of correctly predicted rows from the out of bag sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710\"> Web Link for Ref</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
