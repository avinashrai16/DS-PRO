{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Evaluation Metrics for Logistic Regression\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Confusion matrix\n",
    "\n",
    "A confusion matrix is a table used to evaluate the performance of a classification algorithm. It allows you to visualize the accuracy of predictions by comparing the actual labels with the predicted labels. \n",
    "\n",
    "**TP (True Positive)**: The number of correctly predicted positive cases.\n",
    "\n",
    "**TN (True Negative)**: The number of correctly predicted negative cases.\n",
    "\n",
    "**FP (False Positive)**: The number of incorrectly predicted positive cases (actual negatives that are predicted as positives).\n",
    "\n",
    "**FN (False Negative)**: The number of incorrectly predicted negative cases (actual positives that are predicted as negatives).\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"confusion-matrix.png\" width=\"450\">\n",
    "<img src=\"confusion-matrix-formulas.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Accuracy \n",
    "\n",
    "How many predictions are correctly done from all the data points\n",
    "\n",
    "The ratio of correctly predicted observations to the total observations.\n",
    "Accuracy=TP+TN/TP+TN+FP+FN\n",
    "\n",
    "Where TP = True Positives, TN = True Negatives, FP = False Positives, FN = False Negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Misclassification rate\n",
    "\n",
    "opposite of Accuracy i.e. 1-Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Precision (Positive Predictive Value):\n",
    "\n",
    "The ratio of correctly predicted positive observations to the total predicted positives.\n",
    "\n",
    "Precision=TP / TP+FP\n",
    "\n",
    "​Where TP = True Positives, FP = False Positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Recall (Sensitivity or True Positive Rate):\n",
    "\n",
    "The ratio of correctly predicted positive observations to all observations in the actual class.\n",
    "\n",
    "Recall=TP / TP+FN\n",
    "\n",
    "​Where TP = True Positives, FN = False Negatives.​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    When to use Precision and when to use recall ?\n",
    "\n",
    "**Use case1 : Spam classification:**\n",
    "\n",
    "text -> model -> spam| not spam\n",
    "\n",
    "<u><i>below is the confusion matrix</u></i>\n",
    "\n",
    "TP : mail -spam(class 1)\n",
    "     model - spam(class 1)\n",
    "\n",
    "TN: mail -not spam(class 0)\n",
    "    model - not spam(class 0)\n",
    "\n",
    "FP: mail -not spam(class 0)\n",
    "    model - spam(class 1)\n",
    "\n",
    "FN: mail - spam(class 1)\n",
    "    model - not spam(class 0)\n",
    "\n",
    "So as per the above two cases are not predicted correctly FP and FN\n",
    "\n",
    "<u><i>if business case is required to have mail delivered to Inbox always not in Spam than we need to use Precision as it uses FP.</u></i>\n",
    "\n",
    "Threshold for classification will be increased so that most mail come in Inbox and less is marked as Spam\n",
    "\n",
    "\n",
    "\n",
    "**Use case2 : Diabetic classification:**\n",
    "\n",
    "\n",
    "text -> model -> diabetic| not diabetic\n",
    "\n",
    "<u><i>below is the confusion matrix</u></i>\n",
    "\n",
    "TP : diabetic(class 1)\n",
    "     model - diabetic(class 1)\n",
    "\n",
    "TN: not diabetic(class 0)\n",
    "    model - not diabetic(class 0)\n",
    "\n",
    "FP: not diabetic(class 0)\n",
    "    model - diabetic(class 1)\n",
    "\n",
    "FN: diabetic(class 1)\n",
    "    model - not diabetic(class 0)\n",
    "\n",
    "<u><i>As per the above FN should be priority as not getting diabetic for diabetic may have serious issues later on than we need to use Recall as it uses FN</u></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    F-beta Score\n",
    "\n",
    "<img src=\"f-beta.png\">\n",
    "\n",
    "if FP and FN both are important Beta =1 and F beta is call as F1 score\n",
    "\n",
    "if FP is more important than FN Beta =0.5 and F beta is called as F0.5 score\n",
    "\n",
    "if FN is more important than FP Beta =2.0 and F beta is called as F2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    True Positive Rate :\n",
    "for class 1\n",
    "\n",
    "TP/TP+FN\n",
    "\n",
    "it is same as sensitivity or recall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    False Positive Rate:\n",
    "for class 0\n",
    "\n",
    "FP/FP+TN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    True Negative rate:\n",
    "It is called Specificity\n",
    "\n",
    "TN/TN+FP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ROC AUC (Receiver Operating Characteristic area under Curve):\n",
    "\n",
    "A graphical representation of the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n",
    "Threshold is nothing but cutoff based on which classification done ex: if value of class 1 is more than 0.5 that it will be classified as class1.\n",
    "The area under the ROC curve (AUC-ROC) is often used to summarize the performance. An AUC of 1 indicates a perfect model, while an AUC of 0.5 indicates a model with no discriminatory power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"roc.png\" width=\"450\">\n",
    "\n",
    "<u><i>Higher the area under the curve better the model will be</u></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Precision Recall Accuracy Tradeoff or Sensitivity Specificity Accuracy Tradeoff\n",
    "\n",
    "after potting the lines for each (Precision Recall Accuracy) the crossing point is considered as cutoff rather than fixed one\n",
    "\n",
    "below figure shows only 2 but it will be 3\n",
    "\n",
    "<img src=\"Precision-and-Recall-vs-Cutoff-Threshold.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
