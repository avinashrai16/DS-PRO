{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>AdaBoost- Adaptive Boosting</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost is a boosting algorithm that creates a strong classifier by combining multiple weak classifiers, typically decision stumps. It iteratively adjusts the weights of misclassified instances, giving them more importance in the next iteration. Each weak classifier is trained on the reweighted data, and their predictions are combined through a weighted majority vote. AdaBoost aims to improve classification accuracy while maintaining simplicity and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost with ***decision stumps*** uses simple one-level decision trees as weak learners. Each stump focuses on correcting the errors of its predecessor by adjusting the weights of misclassified instances. The final model is a weighted combination of these stumps, improving overall accuracy while remaining efficient and interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"decision stumps.png\" width=\"250\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"decision-tree vs stumps.png\" width=\"750\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "\n",
    "***Fully grown decision tree (left) vs three decision stumps (right)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"content-box\" id=\"article-start\">\n",
    "<h2 class=\"wp-block-heading\" id=\"Introduction\">Introduction</h2>\n",
    "<p>AdaBoost algorithm, introduced by Freund and Schapire in 1997, revolutionized ensemble modeling. Since its inception, AdaBoost has become a widely adopted technique for addressing binary classification challenges. This powerful algorithm enhances prediction accuracy by transforming a multitude of weak learners into robust, strong learners</p>\n",
    "<p>The principle behind ada boosting algorithms is that we first build a model on the training dataset and then build a second model to rectify the errors present in the first model. This procedure is continued until and unless the errors are minimized and the dataset is predicted correctly. Ada Boosting algorithms work in a similar way, it combines multiple models (weak learners) to reach the final output (strong learners).</p>\n",
    "<h2 class=\"wp-block-heading\" id=\"What_Is_the_AdaBoost_Algorithm?\">What Is the AdaBoost Algorithm?</h2>\n",
    "<p>There are many <a href=\"https://www.analyticsvidhya.com/blog/2022/01/machine-learning-algorithms/\" target=\"_blank\" rel=\"noreferrer noopener\">machine learning algorithms </a>to choose from for your problem statements. One of these algorithms for predictive modeling is called AdaBoost.</p>\n",
    "<p>AdaBoost algorithm, short for Adaptive Boosting, is&nbsp;<strong>a Boosting technique used as an Ensemble Method in Machine Learning</strong>. It is called Adaptive Boosting as the weights are re-assigned to each instance, with higher weights assigned to incorrectly classified instances.</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://av-eks-blogoptimized.s3.amazonaws.com/120932.png\" alt=\"stump | AdaBoost Algorithm\"></figure></div>\n",
    "<p>What this algorithm does is that it builds a model and gives equal weights to all the data points. It then assigns higher weights to points that are wrongly classified. Now all the points with higher weights are given more importance in the next model. It will keep training models until and unless a lower error is received.</p><script src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5229672700622157\" crossorigin=\"anonymous\" async=\"true\"></script><ins class=\"adsbygoogle\" style=\"display: inline-block; width: 728px; height: 0px;\" data-ad-client=\"ca-pub-5229672700622157\" data-ad-slot=\"9793167291\" data-adsbygoogle-status=\"done\" data-ad-status=\"unfilled\"><div id=\"aswift_2_host\" style=\"border: none; height: 0px; width: 728px; margin: 0px; padding: 0px; position: relative; visibility: visible; background-color: transparent; display: inline-block; overflow: hidden; opacity: 0;\"><iframe id=\"aswift_2\" name=\"aswift_2\" browsingtopics=\"true\" style=\"left: 0px; position: absolute; top: 0px; border: 0px; width: 728px; height: 0px;\" sandbox=\"allow-forms allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-top-navigation-by-user-activation\" width=\"728\" height=\"0\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" vspace=\"0\" hspace=\"0\" allowtransparency=\"true\" scrolling=\"no\" allow=\"attribution-reporting; run-ad-auction\" src=\"https://googleads.g.doubleclick.net/pagead/ads?gdpr=0&amp;client=ca-pub-5229672700622157&amp;output=html&amp;h=90&amp;slotname=9793167291&amp;adk=2549625595&amp;adf=1742109659&amp;pi=t.ma~as.9793167291&amp;w=728&amp;abgtt=6&amp;lmt=1718425709&amp;format=728x90&amp;url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2021%2F09%2Fadaboost-algorithm-a-complete-guide-for-beginners%2F&amp;wgl=1&amp;uach=WyJXaW5kb3dzIiwiMTUuMC4wIiwieDg2IiwiIiwiMTI2LjAuNjQ3OC41NyIsbnVsbCwwLG51bGwsIjY0IixbWyJOb3QvQSlCcmFuZCIsIjguMC4wLjAiXSxbIkNocm9taXVtIiwiMTI2LjAuNjQ3OC41NyJdLFsiR29vZ2xlIENocm9tZSIsIjEyNi4wLjY0NzguNTciXV0sMF0.&amp;dt=1718425705670&amp;bpp=16&amp;bdt=1703&amp;idt=755&amp;shv=r20240612&amp;mjsv=m202406120201&amp;ptt=9&amp;saldr=aa&amp;abxe=1&amp;cookie=ID%3Df80f056ce18304ac%3AT%3D1710650499%3ART%3D1718425666%3AS%3DALNI_Mas27PIA4IKwkdAzDaHWS2Fhg1tMA&amp;gpic=UID%3D00000d3bb80603f3%3AT%3D1710650499%3ART%3D1718425666%3AS%3DALNI_MbfeSxK8ExUYFttHnVNsUx-FeFL4Q&amp;eo_id_str=ID%3D519558e7469747b8%3AT%3D1710650499%3ART%3D1718425666%3AS%3DAA-AfjZugGNGxZVE2mBjzQHykn-C&amp;prev_fmts=0x0%2C728x90&amp;nras=1&amp;correlator=8092373379225&amp;frm=20&amp;pv=1&amp;ga_vid=703650892.1710650534&amp;ga_sid=1718425706&amp;ga_hid=33840191&amp;ga_fc=1&amp;u_tz=330&amp;u_his=1&amp;u_h=864&amp;u_w=1536&amp;u_ah=816&amp;u_aw=1536&amp;u_cd=24&amp;u_sd=1&amp;dmc=8&amp;adx=522&amp;ady=2167&amp;biw=1899&amp;bih=869&amp;scr_x=0&amp;scr_y=0&amp;eid=44759876%2C44759927%2C44759842%2C44795921%2C95331689%2C95331833%2C95332924%2C95334511%2C95334527%2C95334573%2C95334819%2C31084600%2C95335245%2C95334054%2C95335290%2C31078663%2C31078665%2C31078668%2C31078670&amp;oid=2&amp;pvsid=1627197815270291&amp;tmod=474569565&amp;uas=0&amp;nvt=1&amp;ref=https%3A%2F%2Fwww.google.com%2F&amp;fc=1920&amp;brdim=0%2C0%2C0%2C0%2C1536%2C0%2C0%2C0%2C1920%2C869&amp;vis=1&amp;rsz=%7C%7CeEbr%7C&amp;abl=CS&amp;pfx=0&amp;fu=0&amp;bc=31&amp;bz=0&amp;td=1&amp;tdf=2&amp;psd=W251bGwsbnVsbCxudWxsLDNd&amp;nt=1&amp;ifi=3&amp;uci=a!3&amp;btvi=1&amp;fsb=1&amp;dtd=3884\" data-google-container-id=\"a!3\" tabindex=\"0\" title=\"Advertisement\" aria-label=\"Advertisement\" data-google-query-id=\"CLrDkb3i3IYDFQtQnQkdzYUIiQ\" data-load-complete=\"true\"></iframe></div></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://av-eks-blogoptimized.s3.amazonaws.com/159381.png\" alt=\"ensemble model | AdaBoost Algorithm\"></figure></div>\n",
    "<p>Let’s take an example to understand this, suppose you built a decision tree algorithm on the Titanic dataset, and from there, you get an accuracy of 80%. After this, you apply a different algorithm and check the accuracy, and it comes out to be 75% for KNN and 70% for Linear Regression.</p>\n",
    "<p>When building different models on the same dataset, we observe variations in accuracy. However, leveraging the power of AdaBoost classifier, we can combine these algorithms to enhance the final predictions. By averaging the results from diverse models, Adaboost allows us to achieve higher accuracy and bolster predictive capabilities effectively.</p>\n",
    "<p>If you want to understand this visually, I strongly recommend you go through this <a href=\"https://www.analyticsvidhya.com/blog/2021/03/introduction-to-adaboost-algorithm-with-python-implementation/\" target=\"_blank\" rel=\"noopener\"><u>article</u></a>.</p>\n",
    "<p>Here we will be more focused on mathematics intuition.</p>\n",
    "<p>There is another ensemble learning algorithm called the gradient ada boosting algorithm. In this algorithm, we try to reduce the error instead of wights, as in AdaBoost. But in this article, we will only be focussing on the mathematical intuition of Adaptive Boosting.</p>\n",
    "<h2 class=\"wp-block-heading\" id=\"Understanding_the_Working_of_the_AdaBoost_Algorithm\">Understanding the Working of the AdaBoost Algorithm</h2>\n",
    "<p>Let’s understand what and how this algorithm works under the hood with the following tutorial.</p>\n",
    "<h3 class=\"wp-block-heading\" id=\"h-step-1-assigning-weights\">Step 1: Assigning Weights</h3>\n",
    "<p>The Image shown below is the actual representation of our dataset. Since the target column is binary, it is a classification problem. First of all, these data points will be assigned some weights. Initially, all the weights will be equal.</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://av-eks-blogoptimized.s3.amazonaws.com/239143.png\" alt=\"data | AdaBoost Algorithm\"></figure></div>\n",
    "<p>The formula to calculate the sample weights is:</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://av-eks-blogoptimized.s3.amazonaws.com/272244.png\" alt=\"formula \"></figure></div>\n",
    "<p>Where N is the total number of data points</p>\n",
    "<p>Here since we have 5 data points, the sample weights assigned will be 1/5.</p>\n",
    "<h3 class=\"wp-block-heading\" id=\"h-step-2-classify-the-samples\">Step 2: Classify the Samples</h3>\n",
    "<p>We start by seeing how well “<em>Gender</em>” classifies the samples and will see how the variables (Age, Income) classify the samples.</p>\n",
    "<p>We’ll create a decision stump for each of the features and then calculate the <strong><em>Gini Index </em></strong>of each tree. The tree with the lowest Gini Index will be our first stump.</p>\n",
    "<p>Here in our dataset, let’s say <strong><em>Gender</em></strong> has the lowest gini index, so it will be our first stump.</p>\n",
    "<h3 class=\"wp-block-heading\" id=\"h-step-3-calculate-the-influence\">Step 3: Calculate the Influence</h3>\n",
    "<p>We’ll now calculate the <strong>“Amount of Say” </strong>or<strong> “Importance” </strong>or <strong>“Influence” </strong>for this classifier in classifying the data points using this formula:</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://av-eks-blogoptimized.s3.amazonaws.com/714195.png\" alt=\"error | AdaBoost Algorithm\"></figure></div>\n",
    "<p>The total error is nothing but the summation of all the sample weights of misclassified data points.</p>\n",
    "<p>Here in our dataset, let’s assume there is 1 wrong output, so our total error will be 1/5, and the alpha (performance of the stump) will be:</p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://av-eks-blogoptimized.s3.amazonaws.com/749268.png\" alt=\"performance of stumps \"></figure>\n",
    "<p><b>Note</b>: Total error will always be between 0 and 1.</p>\n",
    "<p>0 Indicates perfect stump, and 1 indicates horrible stump.</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://av-eks-blogoptimized.s3.amazonaws.com/855446.png\" alt=\"error rate ,adaboost algorithm\"></figure></div>\n",
    "<p>From the graph above, we can see that when there is no misclassification, then we have no error (Total Error = 0), so the “amount of say (alpha)” will be a large number.</p>\n",
    "<p>When the classifier predicts half right and half wrong, then the Total Error = 0.5, and the importance (amount of say) of the classifier will be 0.</p>\n",
    "<p>If all the samples have been incorrectly classified, then the error will be very high (approx. to 1), and hence our alpha value will be a negative integer.</p>\n",
    "<h3 class=\"wp-block-heading\" id=\"h-step-4-calculate-te-and-performance\">Step 4: Calculate TE and Performance</h3>\n",
    "<p>You might be wondering about the significance of calculating the Total Error (TE) and performance of an Adaboost stump. The reason is straightforward – updating the weights is crucial. If identical weights are maintained for the subsequent model, the output will mirror what was obtained in the initial model. </p>\n",
    "<p>The wrong predictions will be given more weight, whereas the correct predictions weights will be decreased. Now when we build our next model after updating the weights, more preference will be given to the points with higher weights.</p>\n",
    "<p>After finding the importance of the classifier and total error, we need to finally update the weights, and for this, we use the following formula:</p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://av-eks-blogoptimized.s3.amazonaws.com/766047.png\" alt=\"net weight sample\"></figure>\n",
    "<p>The amount of, say (alpha) will be <strong><em>negative </em></strong>when the sample is <strong>correctly classified</strong>.</p>\n",
    "<p>The amount of, say (alpha) will be <strong><em>positive</em></strong> when the sample is <strong>miss-classified.</strong></p>\n",
    "<p>There are four correctly classified samples and 1 wrong. Here, the&nbsp;<strong><em>sample weight</em></strong> of that datapoint is<em> 1/5,</em> and the <strong><em>amount of say/performance of the stump&nbsp;</em></strong>of<strong><em> Gender</em></strong> is <em>0.69</em>.</p>\n",
    "<p>New weights for <em>correctly classified</em> samples are:</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://av-eks-blogoptimized.s3.amazonaws.com/171039.png\" alt=\"correctly classified | AdaBoost Algorithm\"></figure></div>\n",
    "<p>For <em>wrongly classified</em> samples, the updated weights will be:</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://av-eks-blogoptimized.s3.amazonaws.com/5473510.png\" alt=\"wrongly classified\"></figure></div>\n",
    "<h4 class=\"wp-block-heading\" id=\"h-note\"><strong>Note</strong></h4>\n",
    "<p>See the sign of alpha when I am putting the values, the <strong>alpha is negative</strong> when the data point is correctly classified, and this <em>decreases the sample weight</em> from 0.2 to 0.1004. It is <strong>positive</strong> when there is <strong>misclassification</strong>, and this will <em>increase the sample weight</em> from 0.2 to 0.3988</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://av-eks-blogoptimized.s3.amazonaws.com/2056611.png\" alt=\"new sample,classification\"></figure></div>\n",
    "<p>We know that the total sum of the sample weights must be equal to 1, but here if we sum up all the new sample weights, we will get 0.8004. To bring this sum equal to 1, we will normalize these weights by dividing all the weights by the total sum of updated weights, which is 0.8004. So, after normalizing the sample weights, we get this dataset, and now the sum is equal to 1.</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://av-eks-blogoptimized.s3.amazonaws.com/1799512.png\" alt=\"TE Performance,adaboost algorithm\"></figure></div>\n",
    "<h3 class=\"wp-block-heading\" id=\"h-step-5-decrease-errors\">Step 5: Decrease Errors</h3>\n",
    "<p>Now, we need to make a new dataset to see if the errors decreased or not. For this, we will remove the “sample weights” and “new sample weights” columns and then, based on the “new sample weights,” divide our data points into buckets.</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://av-eks-blogoptimized.s3.amazonaws.com/5208213.png\" alt=\"data ,decrease errors\"></figure></div>\n",
    "<h3 class=\"wp-block-heading\" id=\"h-step-6-new-dataset\">Step 6: New Dataset</h3>\n",
    "<p>We are almost done. Now, what the algorithm does is selects random numbers from 0-1. Since incorrectly classified records have higher sample weights, the probability of selecting those records is very high.</p>\n",
    "<p>Suppose the 5 random numbers our algorithm take is 0.38,0.26,0.98,0.40,0.55.</p>\n",
    "<p>Now we will see where these random numbers fall in the bucket, and according to it, we’ll make our new dataset shown below.</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://av-eks-blogoptimized.s3.amazonaws.com/2452814.png\" alt=\"output | AdaBoost Algorithm\"></figure></div>\n",
    "<p>This comes out to be our new dataset, and we see the data point, which was wrongly classified, has been selected 3 times because it has a higher weight.</p>\n",
    "<h3 class=\"wp-block-heading\" id=\"h-step-7-repeat-previous-steps\">Step 7: Repeat Previous Steps</h3>\n",
    "<p>Now this act as our new dataset, and we need to repeat all the above steps i.e.</p>\n",
    "<ul>\n",
    "<li>&nbsp;Assign equal weights to all the data points.</li>\n",
    "<li>Find the stump that does the best job classifying the new collection of samples by finding their Gini Index and selecting the one with the lowest Gini index.</li>\n",
    "<li>Calculate the “Amount of Say” and “Total error” to update the previous sample weights.</li>\n",
    "<li>&nbsp;Normalize the new sample weights.</li>\n",
    "</ul>\n",
    "<p>Iterate through these steps until and unless a low training error is achieved.</p>\n",
    "<p>Suppose, with respect to our dataset, we have constructed 3 decision trees (DT1, DT2, DT3) in a&nbsp;<strong><em>sequential manner.</em></strong> If we send our <strong>test data</strong> now, it will pass through all the decision trees, and finally, we will see which class has the majority, and based on that, we will do predictions<br>for our test dataset.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the steps will be same as classification\n",
    "* except using Variance reduction ***for selecting the decision Stump*** in place of Information Gain\n",
    "* For prediction use ***average/mean*** compare to classification where we use majority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
