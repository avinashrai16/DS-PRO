{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting is an ensemble technique that builds a strong predictor by sequentially adding weak learners, typically decision trees, to minimize the residual errors of the previous models. ***Each new model is trained to predict the residuals (errors) of the combined ensemble of all previous models***. This iterative process continues, gradually improving the overall accuracy of the model. ***Gradient Boosting effectively reduces both bias and variance***, leading to high predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What is a Gradient Boosting Algorithm?***\n",
    "\n",
    "Errors play a major role in any machine learning algorithm. There are mainly two types of errors: bias error and variance error. The gradient boost algorithm helps us minimize the bias error of the model. The main idea behind this algorithm is to build models sequentially and these subsequent models try to reduce the errors of the previous model. But how do we do that? How do we reduce the error? This is done by building a new model on the errors or residuals of the previous model.\n",
    "\n",
    "When the target column is continuous, we use Gradient Boosting Regressor whereas when it is a classification problem, we use Gradient Boosting Classifier. The only difference between the two is the “Loss function”. The objective here is to minimize this loss function by adding weak learners using gradient descent. Since it is based on the++ loss function, for regression problems, we’ll have different loss functions like Mean squared error (MSE) and for classification, we will have different functions, like log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"container\">\n",
    "<div class=\"row\">\n",
    "<div class=\"col-lg-8 mx-auto\">\n",
    "<div class=\"author-card d-flex align-items-end mb-5\">\n",
    "<div class=\"content-box\" id=\"article-start\">\n",
    "<h2 class=\"wp-block-heading\" id=\"h-introduction\">Introduction</h2>\n",
    "<p>Prediction models are one of the most commonly used machine learning models. Gradient boosting is a method standing out for its prediction speed and accuracy, particularly with large and complex datasets. From Kaggle competitions to machine learning solutions for business, this algorithm has produced the best results. It is a boosting method and I have talked more about boosting in this <a href=\"https://www.analyticsvidhya.com/blog/2021/09/adaboost-algorithm-a-complete-guide-for-beginners/\">article</a>. It is more popularly known as Gradient Boosting Machine or GBM. In this article, I am going to discuss the math intuition behind the Gradient boosting algorithm.\n",
    "<h2 class=\"wp-block-heading\" id=\"h-what-is-boosting\">What is Boosting?</h2>\n",
    "<p>While studying machine learning you must have come across this term called boosting. It is the most misinterpreted term in the field of Data Science. The principle behind boosting algorithms is first we build a model on the training dataset, then a second model is built to rectify the errors present in the first model. Let me try to explain to you what exactly this means and how this works.</p>\n",
    "<img fetchpriority=\"high\" decoding=\"async\" width=\"650\" src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2021/09/image4.png\" alt=\"Boosting ,gradient boosting\">\n",
    "<p>Suppose you have&nbsp;n&nbsp;data points and 2 output classes (0 and 1). You want to create a model to detect the class of the test data. Now what we do is randomly select observations from the training dataset and feed them to model 1 (M1), we also assume that initially, all the observations have an equal weight which means an equal probability of getting selected.</p>\n",
    "<p>Remember in ensembling techniques the weak learners combine to make a strong model so here M1, M2, M3….Mn, are all weak learners.</p>\n",
    "<p>Since M1 is a weak learner, it will surely misclassify some of the observations. Now, before feeding the observations to M2, we update the weights of the observations which are wrongly classified. You can think of it as a bag that initially contains 10 different color balls, but after some time, some kid takes out his favorite color ball and puts 4 red color balls instead inside the bag. Now, of course, the probability of selecting a red ball is higher.</p>\n",
    "<h4 class=\"wp-block-heading\" id=\"h-other-gradient-boosting-techniques\">Other Gradient Boosting Techniques</h4>\n",
    "<p>This same phenomenon happens in gradient boosting <a href=\"https://www.analyticsvidhya.com/blog/2021/04/how-the-gradient-boosting-algorithm-works/\">techniques</a>, when an observation is wrongly classified, its weight gets updated, and for those which are correctly classified, their weights get decreased. The probability of selecting a wrongly classified observation increases. Hence, in the next model, only those observations get selected that were misclassified in model 1.</p>\n",
    "<p>Similarly, it happens with M2, the wrongly classified weights are again updated and then fed to M3. This procedure is continued until and unless the errors are minimized, and the dataset is predicted correctly. Now when the new datapoint comes in (Test data) it passes through all the models (weak learners) and the class that gets the highest vote is the output for our test data.</p>\n",
    "<h2 class=\"wp-block-heading\" id=\"h-what-is-gradient-boosting\">What is Gradient Boosting?</h2>\n",
    "<p>Gradient boosting is a machine learning ensemble technique that combines the predictions of multiple weak learners, typically decision trees, sequentially. It aims to improve overall predictive performance by optimizing the model’s weights based on the errors of previous iterations, gradually reducing prediction errors and enhancing the model’s accuracy. This is most commonly used for linear regression.</p>\n",
    "<img decoding=\"async\" width=\"650\" src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2024/01/Gradient-Boosting-tree-scaled.jpg\" alt=\"Gradient Boosting tree\" class=\"wp-image-140356\">\n",
    "<h2 class=\"wp-block-heading\" id=\"h-what-is-adaboost-algorithm\">What is AdaBoost Algorithm?</h2>\n",
    "<p>Before getting into the details of the gradient boosting algorithm, we must have some knowledge about the AdaBoost Algorithm which is again a boosting method. This algorithm starts by building a decision stump and then assigning equal weights to all the data points. Then it increases the weights for all the points that are misclassified and lowers the weight for those that are easy to classify or are correctly classified. A new decision stump is made for these weighted data points. The idea behind this is to improve the predictions made by the first stump. I have talked more about this algorithm here. Read this <a href=\"https://www.analyticsvidhya.com/blog/2021/09/adaboost-algorithm-a-complete-guide-for-beginners/\">article</a> before starting this algorithm to get a better understanding.</p>\n",
    "<p><u>The main difference between these two algorithms is that gradient boosting has a fixed base estimator i.e., decision trees whereas in AdaBoost we can change the base estimator according to our needs</u>.</p>\n",
    "<h2 class=\"wp-block-heading\" id=\"h-what-is-a-gradient-boosting-algorithm\">What is a Gradient Boosting Algorithm?</h2>\n",
    "<p>Errors play a major role in any machine learning algorithm. There are mainly two types of errors: bias error and variance error. The gradient boost algorithm helps us minimize the bias error of the model. The main idea behind this algorithm is to build models sequentially and these subsequent models try to reduce the errors of the previous model. But how do we do that? How do we reduce the error? This is done by building a new model on the errors or residuals of the previous model.</p>\n",
    "<p>When the target column is continuous, we use Gradient Boosting Regressor whereas when it is a classification problem, we use Gradient Boosting Classifier. The only difference between the two is the <em>“Loss function”</em>. The objective here is to minimize this loss function by adding weak learners using gradient descent. Since it is based on the++ loss function, for regression problems, we’ll have different loss functions like Mean squared error (MSE) and for classification, we will have different functions, like log-likelihood.</p>\n",
    "<h2 class=\"wp-block-heading\" id=\"h-understanding-gradient-boosting-algorithm-with-an-example\">Understanding Gradient Boosting Algorithm with An Example</h2>\n",
    "<p>Let’s understand the intuition behind the gradient boosting algorithm with the help of<a href=\"https://www.analyticsvidhya.com/blog/2021/04/how-the-gradient-boosting-algorithm-works/\"> an example</a>. Here our target column is continuous hence we will use gradient boosting regressor.</p>\n",
    "<p>Following is a sample from a random dataset where we have to predict the car price based on various features. The target column is price and other features are independent features.</p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/415591.png\" alt=\"data example,gradient boosting algorithm\"></figure>\n",
    "<h4 class=\"wp-block-heading\" id=\"h-step-1-build-a-base-model\">Step 1: Build a Base Model</h4>\n",
    "<p>The first step in gradient boosting is to build a base model to predict the observations in the training dataset. For simplicity, we take an average of the target column and assume that to be the predicted value as shown below:</p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/789352.png\" alt=\"base model prediction | Gradient Boosting Algorithm\"></figure>\n",
    "<p>Why did I say we take the average of the target column? Well, there is math involved in this. Mathematically the first step can be written as:</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/798903.png\" alt=\"argmin,gradient boosting\"></figure></div>\n",
    "<p>Looking at this may give you a headache, but don’t worry we will try to understand what is written here.</p>\n",
    "<p>Here L is our loss function,<br>Gamma is our predicted value, and<br>arg min means we have to find a predicted value/gamma for which the loss function is minimum.</p>\n",
    "<p>Since the target column is continuous our loss function will be:</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/896724.png\" alt=\"loss function | Gradient Boosting Algorithm\"></figure></div>\n",
    "<p>Here y<sub>i</sub> is the observed value, and gamma is the predicted value.</p>\n",
    "<p>Now we need to find a minimum value of gamma such that this loss function is minimum. We all have studied how to find minima and maxima in our 12th grade. Did we use it to differentiate this loss function and then put it equal to 0 right? Yes, we will do the same here.</p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/848675.png\" alt=\"differentiate loss function\"></figure>\n",
    "<p>Let’s see how to do this with the help of our example. Remember that y_i is our observed value and gamma_i is our predicted value, by plugging the values in the above formula we get:</p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/842636.png\" alt=\"plug values | Gradient Boosting Algorithm\"></figure>\n",
    "<p>We end up over an average of the observed car price and this is why I asked you to take the average of the target column and assume it to be your first prediction.</p>\n",
    "<p>Hence for gamma=14500, the loss function will be minimum so this value will become our prediction for the&nbsp;<em>base model</em>.</p>\n",
    "<h4 class=\"wp-block-heading\">Step 2: Compute Pseudo Residuals</h4>\n",
    "<p>The next step is to calculate the pseudo residuals which are (observed value – predicted value).</p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/153937.png\" alt=\"calculate residual | Gradient Boosting Algorithm\"></figure>\n",
    "<p>Again the question comes why only observed – predicted? Everything is mathematically proven. Let’s see where this formula comes from. This step can be written as:</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/869318.png\" alt=\"rewrite | Gradient Boosting Algorithm\"></figure></div>\n",
    "<p>Here F(x<sub>i</sub>) is the previous model and m is the number of DT made.</p>\n",
    "<p>We are just taking the derivative of loss function w.r.t the predicted value and we have already calculated this derivative:</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/300499.png\" alt=\"calculate derivative \"></figure></div>\n",
    "<p>If you see the formula of residuals above, we see that the derivative of the loss function is multiplied by a negative sign, so now we get:</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/8311610.png\" alt=\"error\"></figure></div>\n",
    "<p>The predicted value here is the prediction made by the previous model. In our example the prediction made by the previous model (initial base model prediction) is 14500, to calculate the residuals our formula becomes:</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/4757811.png\" alt=\"calculating errors\"></figure></div>\n",
    "<h4 class=\"wp-block-heading\">Step 3: Build a Model on Calculated Residuals</h4>\n",
    "<p>In the next step, we will build a model on these pseudo residuals and make predictions. Why do we do this? Because we want to minimize these residuals minimizing the residuals will eventually improve our model accuracy and prediction power. So, using the Residual as a target and the original feature Cylinder number, cylinder height, and Engine location we will generate new predictions. Note that the predictions, in this case, will be the error values, not the predicted car price values since our target column is an error now.</p>\n",
    "<p>Let’s say <b>h<sub>m</sub>(x)</b> is our DT made on these residuals.</p>\n",
    "<h4 class=\"wp-block-heading\">Step 4: Compute Decision Tree Output</h4>\n",
    "<p>In this step, we find the output values for each leaf of our decision tree. That means there might be a case where 1 leaf gets more than 1 residual, hence we need to find the final output of all the leaves. To find the output we can simply take the average of all the numbers in a leaf, doesn’t matter if there is only 1 number or more than 1.</p>\n",
    "<p>Let’s see why we take the average of all the numbers. Mathematically this step can be represented as:</p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/5331112.png\" alt=\"minimum error\"></figure>\n",
    "<p>Here <b>h<sub>m</sub>(x<sub>i</sub>)</b> is the DT made on residuals and <b>m</b> is the number of DT. When m=1 we are talking about the 1st DT and when it is “<b>M</b>” we are talking about the last DT.</p>\n",
    "<p>The output value for the leaf is the value of gamma that minimizes the Loss function. The left-hand side <em>“Gamma”</em> is the output value of a particular leaf. On the right-hand side <em>[F m-1 (x i )+ƴh m (x i ))]</em> is similar to step 1 but here the difference is that we are taking previous predictions whereas earlier there was no previous prediction.</p>\n",
    "<h5 class=\"wp-block-heading\" id=\"h-example-of-calculating-regression-tree-output\">Example of Calculating Regression Tree Output</h5>\n",
    "<p>Let’s understand this even better with the help of an example. Suppose this is our regressor tree:</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/6460613.png\" alt=\"tree | Gradient Boosting Algorithm\"></figure></div>\n",
    "<p>We see 1<sup>st</sup>&nbsp;residual goes in R<sub>1,1&nbsp;&nbsp;</sub>,2<sup>nd</sup>&nbsp;and 3<sup>rd</sup>&nbsp;residuals go in R<sub>2,1 </sub>and 4<sup>th</sup> residual goes in R<sub>3,1 </sub>.</p>\n",
    "<p>Let’s calculate the output for the first leave that is R<sub>1,1</sub></p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/5422314.png\" alt=\"output first leave | Gradient Boosting Algorithm\"></figure>\n",
    "<p>Now we need to find the value for gamma for which this function is minimum. So we find the derivative of this equation w.r.t gamma and put it equal to 0.</p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/7601415.png\" alt=\"gamma\"></figure>\n",
    "<p>Hence the leaf R<sub>1,1 </sub>has an output value of -2500. Now let’s solve for the R<sub>2,1</sub>.</p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/5683816.png\" alt=\"\"></figure>\n",
    "<p>Let’s take the derivative to get the minimum value of gamma for which this function is minimum:</p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/5083517.png\" alt=\"minimum | Gradient Boosting Algorithm\"></figure>\n",
    "<p>We end up with the <b>average</b> of the residuals in the leaf R<sub>2,1</sub> . Hence if we get any leaf with more than 1 residual, we can simply find the average of that leaf and that will be our final output.</p>\n",
    "<p>Now after calculating the output of all the leaves, we get:</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/5762213.1.png\" alt=\"output for all leaves | Gradient Boosting Algorithm\"></figure></div>\n",
    "<h4 class=\"wp-block-heading\">Step 5: Update Previous Model Predictions</h4>\n",
    "<p>This is finally the last step where we have to update the predictions of the previous model. It can be updated as:</p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/8876618.png\" alt=\"update model | Gradient Boosting Algorithm\"></figure>\n",
    "<p>where m is the number of decision trees made.</p>\n",
    "<p>Since we have just started building our model so our m=1. Now to make a new DT our new predictions will be:</p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/2331519.png\" alt=\"new predictions\"></figure>\n",
    "<p>Here <b>F<sub>m-1</sub>(x)</b> is the prediction of the base model (previous prediction) since F<sub>1-1=0 </sub>, F<sub>0</sub> is our base model hence the previous prediction is 14500.</p>\n",
    "<p><b>nu</b> is the <i><b>learning rate</b></i> that is usually selected between <b>0-1</b>. It reduces the effect each tree has on the final prediction, and this improves accuracy in the long run. Let’s take <i><b>nu=0.1</b></i> in this example.</p>\n",
    "<p>H<sub>m</sub>(x) is the recent DT made on the residuals.</p>\n",
    "<p>Let’s calculate the new prediction now:</p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/7910720.png\" alt=\"calculate new predictions | Gradient Boosting Algorithm\"></figure>\n",
    "<p>Suppose we want to find a prediction of our first data point which has a car height of 48.8. This data point will go through this decision tree and the output it gets will be multiplied by the learning rate and then added to the previous prediction.</p>\n",
    "<p>Now let’s say m=2 which means we have built 2 decision trees and now we want to have new predictions.</p>\n",
    "<p>This time we will add the previous prediction that is <b>F<sub>1</sub>(x)</b> to the new DT made on residuals. We will iterate through these steps again and again till the<b><i> <b>loss is negligible.</b></i></b></p>\n",
    "<p>I am taking a hypothetical example here just to<br>make you understand how this predicts for a new dataset:</p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/1860022.png\" alt=\"predictions for new data\"></figure>\n",
    "<p>If a new data point comes, say, height = 1.40, it’ll go through all the trees and then will give the prediction. Here we have only 2 trees hence the datapoint will go through these 2 trees and the final output will be <b>F<sub>2</sub>(x)</b>.</p>\n",
    "<h2 class=\"wp-block-heading\" id=\"h-what-is-a-gradient-boosting-classifier\">What is a Gradient Boosting Classifier?</h2>\n",
    "<p>A gradient-boosting classifier is used when the target column is binary. All the steps explained in the gradient boosting regressor are used here, the only difference is we change the loss function. Earlier we used Mean squared error when the target column was continuous but this time, we will use log-likelihood as our loss function.</p>\n",
    "<p>Let’s see how this loss function works, to read more about log-likelihood I recommend you to go through this <a href=\"https://editor.analyticsvidhya.com/preview/index?article_id=9848\"><u>article</u></a> where I have given each detail you need to understand this.</p>\n",
    "<p>The loss function for the classification problem is given below:</p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/885012.png\" alt=\"loss function\"></figure>\n",
    "<p>Our first step in the gradient boosting algorithm was to initialize the model with some constant value, there we used the average of the target column but here we’ll use log(odds) to get that constant value. The question comes why log(odds)?</p>\n",
    "<p>When we differentiate this loss function, we will get a function of log(odds) and then we need to find a value of log(odds) for which the loss function is minimum.</p>\n",
    "<h3 class=\"wp-block-heading\" id=\"h-how-does-it-work\">How Does it Work?</h3>\n",
    "<p>Are you confused right? now Okay let’s see how it works:</p>\n",
    "<p>Let’s first transform this loss function so that it is a function of log(odds), I’ll tell you later why we did this transformation.</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/545223.png\" alt=\"function of loss | Gradient Boosting \"></figure></div>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/217424.png\" alt=\"log(odds) | Gradient Boosting Algorithm\"></figure>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/188425.png\" alt=\"calculate loss | Gradient Boosting \"></figure></div>\n",
    "<p>Now this is our loss function, and we need to minimize it, for this, we take the derivative of this w.r.t to log(odds) and then put it equal to 0,</p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/715546.png\" alt=\"derivative of loss | Gradient Boosting Algorithm\"></figure>\n",
    "<p>Here, ‘y’ denotes the observed values.</p>\n",
    "<p>You must be wondering why we transformed the loss function into a function of log(odds). Sometimes it is easy to use the function of log(odds), and sometimes it’s easy to use the function of predicted probability “p”.</p>\n",
    "<p>It isn’t compulsory to transform the loss function, we did this just to have easy calculations.</p>\n",
    "<p>Hence the minimum value of this loss function will be our first prediction (base model prediction)</p>\n",
    "<p>Now in the gradient boosting regressor, our next step was to calculate the pseudo residuals where we multiplied the derivative of the loss function with <em>-1</em>. We will do the same but now the loss function is different, and we are dealing with the probability of an outcome now.</p>\n",
    "<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/927797.png\" alt=\"gradient boosting \"></figure>\n",
    "<p>After finding the residuals we can build a decision tree with all independent variables and target variables as “Residuals”.</p>\n",
    "<p>Now when we have our first decision tree, we find the final output of the leaves because there might be a case where a leaf gets more than 1 residuals, so we need to calculate the final output value. The math behind this step is out of the scope of this article so I will mention the direct formula to calculate the output of a leaf:</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img decoding=\"async\" src=\"https://editor.analyticsvidhya.com/uploads/605878.png\" alt=\"equation, gradient boosting classifier\"></figure></div>\n",
    "<p>Finally, we are ready to get new predictions by adding our base model with the new tree we made on residuals.</p>\n",
    "<h2 class=\"wp-block-heading\" id=\"h-implementation-of-gbm-using-scikit-learn\">Implementation of GBM Using scikit-learn</h2>\n",
    "<p>For the implementation of GBM on a dataset, we will be using the Income Evaluation dataset, which has information about an individual’s personal life and an output of 50K or &lt;=50. The dataset can be found here (<a href=\"https://www.kaggle.com/lodetomasi1995/income-classification\"><strong>https://www.kaggle.com/lodetomasi1995/income-classification</strong></a>)</p>\n",
    "<p>The task here is to classify the income of an individual when given the required inputs about his personal life.</p>\n",
    "<p>First, let’s import all required libraries.</p>\n",
    "<pre class=\"wp-block-code\"><code data-highlighted=\"yes\" class=\"hljs language-kotlin\"># Import all relevant librariesfrom sklearn.ensemble <span class=\"hljs-keyword\">import</span> GradientBoostingClassifierimport numpy <span class=\"hljs-keyword\">as</span> npimport pandas <span class=\"hljs-keyword\">as</span> pdfrom sklearn.preprocessing <span class=\"hljs-keyword\">import</span> StandardScalerfrom sklearn.model_selection <span class=\"hljs-keyword\">import</span> train_test_splitfrom sklearn.metrics <span class=\"hljs-keyword\">import</span> accuracy_score, confusion_matrixfrom sklearn <span class=\"hljs-keyword\">import</span> preprocessingimport warningswarnings.filterwarnings(<span class=\"hljs-string\">\"ignore\"</span>)Now let’s read the dataset and look at the columns to understand the information better.</code></pre>\n",
    "<pre class=\"wp-block-code\"><code data-highlighted=\"yes\" class=\"hljs language-bash\"><span class=\"hljs-built_in\">df</span>&nbsp;=&nbsp;pd.read_csv(<span class=\"hljs-string\">'income_evaluation.csv'</span>)\n",
    "df.head()</code></pre>\n",
    "<figure class=\"wp-block-image size-full\"><img decoding=\"async\" width=\"1548\" height=\"369\" src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2024/01/image-54.png\" alt=\"gradient boosting \" class=\"wp-image-140341\"></figure>\n",
    "<p>I have already done the data preprocessing part and you can see the whole code <a href=\"https://github.com/AnshulSaini17/Income_evaluation/blob/main/Income_Evalutation.ipynb\">here</a>. Here my main aim is to tell you how to implement this in Python. Now for training and testing our model, the data has to be divided into train and test sets.</p>\n",
    "<p>We will also scale the data to lie between 0 and 1.</p>\n",
    "<pre class=\"wp-block-code\"><code data-highlighted=\"yes\" class=\"hljs language-bash\"><span class=\"hljs-comment\"># Split dataset into test and train data</span></code></pre>\n",
    "<pre class=\"wp-block-code\"><code data-highlighted=\"yes\" class=\"hljs language-bash\">X_train, X_test, y_train, y_test = train_test_split(df.drop(‘income’, axis=1),<span class=\"hljs-built_in\">df</span>[‘income’], test_size=0.2)</code></pre>\n",
    "<p>Now let’s go ahead with defining the Gradient Boosting Classifier along with its hyperparameters. Next, we will fit this model into the training data.</p>\n",
    "<pre class=\"wp-block-code\"><code data-highlighted=\"yes\" class=\"hljs language-makefile\"><span class=\"hljs-comment\"># Define Gradient Boosting Classifier with hyperparameters</span>\n",
    "\n",
    "gbc=GradientBoostingClassifier(n_estimators=500,learning_rate=0.05,random_state=100,max_features=5 )\n",
    "\n",
    "<span class=\"hljs-comment\"># Fit train data to GBC</span>\n",
    "\n",
    "gbc.fit(X_train,y_train)</code></pre>\n",
    "<p>The model has been trained and we can now observe the outputs as well.</p>\n",
    "<p>Below, you can see the confusion matrix of the model, which gives a report of the number of classifications and misclassifications.</p>\n",
    "<pre class=\"wp-block-code\"><code data-highlighted=\"yes\" class=\"hljs language-bash\"><span class=\"hljs-comment\"># Confusion matrix will give number of correct and incorrect classifications</span>\n",
    "<span class=\"hljs-built_in\">print</span>(confusion_matrix(y_test, gbc.predict(X_test)))</code></pre>\n",
    "<pre class=\"wp-block-preformatted\"><img loading=\"lazy\" decoding=\"async\" width=\"177\" height=\"73\" class=\"aligncenter wp-image-85151 size-full\" src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2021/09/image5.png\" alt=\"Confusion Matrix\"></pre>\n",
    "<p>The number of misclassifications by the Gradient Boosting Classifier is 1334, compared to 8302 correct classifications. The model has performed decently.<br>Let’s check the accuracy:</p>\n",
    "<pre class=\"wp-block-code\"><code data-highlighted=\"yes\" class=\"hljs language-perl\"><span class=\"hljs-comment\"># Accuracy of model</span>\n",
    "\n",
    "<span class=\"hljs-keyword\">print</span>(<span class=\"hljs-string\">\"GBC accuracy is %2.2f\"</span> % accuracy_score( \n",
    "     y_test, gbc.predict(X_test)))</code></pre>\n",
    "<pre class=\"wp-block-preformatted\"><img loading=\"lazy\" decoding=\"async\" width=\"258\" height=\"33\" class=\"aligncenter wp-image-85148 size-full\" src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2021/09/image8.png\" alt=\"Output Snippet\"></pre>\n",
    "<p>Let’s check the classification report also:</p>\n",
    "<pre class=\"wp-block-code\"><code data-highlighted=\"yes\" class=\"hljs language-javascript\"><span class=\"hljs-keyword\">from</span> sklearn.<span class=\"hljs-property\">metrics</span> <span class=\"hljs-keyword\">import</span> classification_report </code></pre>\n",
    "<pre class=\"wp-block-code\"><code data-highlighted=\"yes\" class=\"hljs language-ini\"><span class=\"hljs-attr\">pred</span>=gbc.predict(X_test) </code></pre>\n",
    "<pre class=\"wp-block-code\"><code data-highlighted=\"yes\" class=\"hljs language-scss\"><span class=\"hljs-built_in\">print</span>(classification_report(y_test, pred))</code></pre>\n",
    "<p>The accuracy is 86%, which is pretty good but this can be improved by tuning the hyperparameters or processing the data to remove outliers.<br>This however gives us the basic idea behind gradient boosting and its underlying working principles.</p>\n",
    "<p>The accuracy is 86%, which is pretty good but this can be improved by tuning the hyperparameters or processing the data to remove outliers.<br>This however gives us the basic idea behind gradient boosting and its underlying working principles.</p>\n",
    "<h2 class=\"wp-block-heading\" id=\"h-parameter-tuning-in-gradient-boosting-gbm-in-python\">Parameter Tuning in Gradient Boosting (GBM) in Python</h2>\n",
    "<h4 class=\"wp-block-heading\">Tuning n_estimators and Learning rate</h4>\n",
    "<p>n_estimators is the number of trees (weak learners) that we want to add in the model. There are no optimum values for the learning rate as low values always work better, given that we train on a sufficient number of trees. A high number of trees can be computationally expensive. That’s why I have taken a few number of trees here.</p>\n",
    "<pre class=\"wp-block-code\"><code data-highlighted=\"yes\" class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> sklearn.model_selection <span class=\"hljs-keyword\">import</span> GridSearchCV\n",
    "\n",
    "grid = {\n",
    "\n",
    "    <span class=\"hljs-string\">'learning_rate'</span>:[<span class=\"hljs-number\">0.01</span>,<span class=\"hljs-number\">0.05</span>,<span class=\"hljs-number\">0.1</span>],\n",
    "\n",
    "    <span class=\"hljs-string\">'n_estimators'</span>:np.arange(<span class=\"hljs-number\">100</span>,<span class=\"hljs-number\">500</span>,<span class=\"hljs-number\">100</span>),\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "gb_cv = GridSearchCV(gb, grid, cv = <span class=\"hljs-number\">4</span>)\n",
    "\n",
    "gb_cv.fit(X_train,y_train)\n",
    "\n",
    "<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Best Parameters:\"</span>,gb_cv.best_params_)\n",
    "\n",
    "<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Train Score:\"</span>,gb_cv.best_score_)\n",
    "\n",
    "<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Test Score:\"</span>,gb_cv.score(X_test,y_test))</code></pre>\n",
    "<pre class=\"wp-block-preformatted\"><img loading=\"lazy\" decoding=\"async\" width=\"741\" height=\"96\" class=\"aligncenter wp-image-85154 size-full\" src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2021/09/image2.png\" alt=\"Output Snippet\"></pre>\n",
    "<p>We see the accuracy increased from 86 to 89 after tuning n_estimators and learning rate. Also the “true positive” and the “true negative” rate improved.</p>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img loading=\"lazy\" decoding=\"async\" width=\"664\" height=\"230\" src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2021/09/image1.png\" alt=\"Classification Report\" class=\"wp-image-85155\"></figure></div>\n",
    "<p>We can also tune max_depth parameter which you must have heard in decision trees and random forests.</p>\n",
    "<pre class=\"wp-block-code\"><code data-highlighted=\"yes\" class=\"hljs language-bash\">grid = {<span class=\"hljs-string\">'max_depth'</span>:[2,3,4,5,6,7] }\n",
    "\n",
    "gb = GradientBoostingClassifier(learning_rate=0.1,n_estimators=400)\n",
    "\n",
    "gb_cv = GridSearchCV(gb, grid, cv = 4)\n",
    "\n",
    "gb_cv.fit(X_train,y_train)\n",
    "\n",
    "<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Best Parameters:\"</span>,gb_cv.best_params_)\n",
    "\n",
    "<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Train Score:\"</span>,gb_cv.best_score_)\n",
    "\n",
    "<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Test Score:\"</span>,gb_cv.score(X_test,y_test))</code></pre>\n",
    "<div class=\"wp-block-image\">\n",
    "<figure class=\"aligncenter\"><img loading=\"lazy\" decoding=\"async\" width=\"437\" height=\"100\" src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2021/09/image3.png\" alt=\"Output Snippet,gradient boosting\" class=\"wp-image-85153\"></figure></div>\n",
    "<p>The accuracy increased even more when we tuned the parameter “max_depth”.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier\n",
    "\n",
    "* Base prediction 0.5\n",
    "* Instead of variance reduction use Information gain"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
