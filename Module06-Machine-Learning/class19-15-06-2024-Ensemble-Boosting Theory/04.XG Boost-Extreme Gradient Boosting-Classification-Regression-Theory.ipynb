{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost (Extreme Gradient Boosting) is a powerful gradient boosting algorithm known for its speed and performance:\n",
    "1. **Tree Ensemble Method**: XGBoost builds an ensemble of decision trees sequentially, optimizing a differentiable loss function at each step.\n",
    "2. **Regularization**: It incorporates regularization techniques to control overfitting, such as shrinkage (learning rate) and pruning.\n",
    "3. **Parallel Processing**: Utilizes parallel processing and tree pruning to optimize training speed and model performance.\n",
    "4. **Wide Applicability**: Widely used in structured/tabular data tasks like classification, regression, and ranking problems due to its robustness and effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=w-_vmVfpssg - Regression\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=gPciUPwWJQQ - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"ast-post-format- single-layout-1\">\t\t\n",
    "<p>XGBoost is a fast and powerful machine learning algorithm, which has become a popular choice for online machine learning competitions due to its high efficacy. Its complete moniker is Extreme Gradient Boosting. XGBoost is an enhanced implementation of Gradient Boosting in terms of both speed and performance.</p>\n",
    "<p>XGBoost is a high-performance decision-tree-based ensemble learning method usually used for supervised learning. In Ensemble Learning, results from different individual models are combine in order to make an optimize prediction. Instead of depending on a single model’s outcome, the Ensemble learning method makes a decision by aggregating results from different models to put forth a model with better accuracy.</p>\n",
    "<h4 class=\"wp-block-heading\"><strong>TYPES OF ENSEMBLE METHODS</strong></h4>\n",
    "<ol><li>Bagging — In bootstrap aggregating(BAGGing), numerous models are Use Up parallel using randomly training data subsets. These models are then combine based on majority votes of their decisions.</li><li>Boosting — Similar to bagging but instead of parallel, the models are use up in series. For each successive model, the weights are adjust by depending on the performance of previous models.&nbsp;</li></ol>\n",
    "<h3 class=\"wp-block-heading\">XGBoost</h3>\n",
    "<p>XGBoost is an optimized enhancement of Gradient Boosting. In boosting, weights are added to the model based on the residuals. However, in gradient boosted the loss function is optimized to correct errors made by previous models. XGBoost introduces new features to gradient boosting like regularization, tree pruning, and parallel processing. To clearly understand the XGboost algorithm let’s take a look at XGboost in regression as well as classification, with examples.&nbsp;</p>\n",
    "<h4 class=\"wp-block-heading\"><strong>1. REGRESSION</strong></h4>\n",
    "<p>For regression, let’s try to predict the marks of students based on the number of hours studied. Initially, let us consider the average Marks to find the residuals. The average of the five marks is 40, which will be the prediction of the base model. Consequently, the residuals for the first five readings are -19, 7, -13, 35 and -10, respectively.</p>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter\"><noscript><img decoding=\"async\" src=\"https://lh3.googleusercontent.com/6KY-7Vl0pYz2LvXMxG4L5l3VFjN0392Drjj3CLNXHWMiF8WPCC49xB-AYXY9DZLaQOoiTx9BnkBrlfQ9JUO1s75_hxxQ4dBpe8dJrHE7CKoeaTi_SRCGi8yPvQSCkHwYQLm9TbS4\" alt=\"\"/></noscript><img class=\" lazyloaded\" decoding=\"async\" src=\"https://lh3.googleusercontent.com/6KY-7Vl0pYz2LvXMxG4L5l3VFjN0392Drjj3CLNXHWMiF8WPCC49xB-AYXY9DZLaQOoiTx9BnkBrlfQ9JUO1s75_hxxQ4dBpe8dJrHE7CKoeaTi_SRCGi8yPvQSCkHwYQLm9TbS4\" data-src=\"https://lh3.googleusercontent.com/6KY-7Vl0pYz2LvXMxG4L5l3VFjN0392Drjj3CLNXHWMiF8WPCC49xB-AYXY9DZLaQOoiTx9BnkBrlfQ9JUO1s75_hxxQ4dBpe8dJrHE7CKoeaTi_SRCGi8yPvQSCkHwYQLm9TbS4\" alt=\"\"></figure></div>\n",
    "<p>We use the residuals to construct the decision tree with splitting criteria say&nbsp; Hours studied greater than 3.3, then calculate the similarity scores for the root and leaf nodes. λ in the equation is the regularization parameter.</p>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><noscript><img decoding=\"async\" src=\"https://lh5.googleusercontent.com/FS4M5zu0_nEZywfH4gZWonw_YHeSHF2ALjyf-Gf3DldqzDrFfVqFfXTUiXVMdP9A-63kFtlmGiWzyBZXTd-g_IZlE6-WvhqbSMtFsUL5ag6t8uQO2RtqYyWQNl0XxX532AuLZ6_k\" alt=\"\" width=\"330\" height=\"199\"/></noscript><img class=\" lazyloaded\" decoding=\"async\" src=\"https://lh5.googleusercontent.com/FS4M5zu0_nEZywfH4gZWonw_YHeSHF2ALjyf-Gf3DldqzDrFfVqFfXTUiXVMdP9A-63kFtlmGiWzyBZXTd-g_IZlE6-WvhqbSMtFsUL5ag6t8uQO2RtqYyWQNl0XxX532AuLZ6_k\" data-src=\"https://lh5.googleusercontent.com/FS4M5zu0_nEZywfH4gZWonw_YHeSHF2ALjyf-Gf3DldqzDrFfVqFfXTUiXVMdP9A-63kFtlmGiWzyBZXTd-g_IZlE6-WvhqbSMtFsUL5ag6t8uQO2RtqYyWQNl0XxX532AuLZ6_k\" alt=\"\" width=\"330\" height=\"199\"></figure></div>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><noscript><img decoding=\"async\" src=\"https://lh6.googleusercontent.com/Bg_NwoxW0bWFUPVPX7gXZFGsNyhn0x_szGxbfmZHFpBWx4OdZ1eFGVj24TYQRxpnNilbOeXfBWORk0RklfoJ60klrjg-5Qpj5WFZfIGNKDKJjclQXXzmnNoONKx9KytssNfUYKaf\" alt=\"\" width=\"392\" height=\"214\"/></noscript><img class=\" lazyloaded\" loading=\"lazy\" decoding=\"async\" src=\"https://lh6.googleusercontent.com/Bg_NwoxW0bWFUPVPX7gXZFGsNyhn0x_szGxbfmZHFpBWx4OdZ1eFGVj24TYQRxpnNilbOeXfBWORk0RklfoJ60klrjg-5Qpj5WFZfIGNKDKJjclQXXzmnNoONKx9KytssNfUYKaf\" data-src=\"https://lh6.googleusercontent.com/Bg_NwoxW0bWFUPVPX7gXZFGsNyhn0x_szGxbfmZHFpBWx4OdZ1eFGVj24TYQRxpnNilbOeXfBWORk0RklfoJ60klrjg-5Qpj5WFZfIGNKDKJjclQXXzmnNoONKx9KytssNfUYKaf\" alt=\"\" width=\"392\" height=\"214\"></figure></div>\n",
    "<p>To begin with, let λ=0. The similarity score for the root is calculate to be 0. For the two nodes, the similarity scores are 341.33 and 512.</p>\n",
    "<p>We then calculate the gain which in this case is 853.33.</p>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter\"><noscript><img decoding=\"async\" src=\"https://lh6.googleusercontent.com/5uUDzbMyTVTN96drb94twqlWwySUlWxLKu2gtj-Avn-paWAJMeBx7mz6yUE36o8IQceSoO4jyH7k1TnX50QRNlN9GxPUbbn9Vj6fzn-25nECwZ-n8POO1dYkRTFmkdRgZWkwIbV7\" alt=\"\"/></noscript><img class=\" lazyloaded\" decoding=\"async\" src=\"https://lh6.googleusercontent.com/5uUDzbMyTVTN96drb94twqlWwySUlWxLKu2gtj-Avn-paWAJMeBx7mz6yUE36o8IQceSoO4jyH7k1TnX50QRNlN9GxPUbbn9Vj6fzn-25nECwZ-n8POO1dYkRTFmkdRgZWkwIbV7\" data-src=\"https://lh6.googleusercontent.com/5uUDzbMyTVTN96drb94twqlWwySUlWxLKu2gtj-Avn-paWAJMeBx7mz6yUE36o8IQceSoO4jyH7k1TnX50QRNlN9GxPUbbn9Vj6fzn-25nECwZ-n8POO1dYkRTFmkdRgZWkwIbV7\" alt=\"\"></figure></div>\n",
    "<p>After the gain is calculate, the auto tree pruning is complete. For this purpose, we use the gamma parameter in XGboost regression. <u>If the gain is less than the gamma value then the branch is cut and no further splitting takes place else splitting continues</u>. If the value of gamma is more, more pruning takes place. The learning rate in XGboost is in use to know the convergence of the model. For this algorithm, it is refer to as eta and the default value is set to 0.3.&nbsp;</p>\n",
    "<p>Now we find the new prediction for the data.</p>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><noscript><img loading=\"lazy\" decoding=\"async\" src=\"https://lh3.googleusercontent.com/OtpTAooVApN0aWcsKyPNY2pvJ-GrfR9tavfa0CGj3WyFKfehHvllrRWRi7lAXVDg3j-GXSF-Pn8zrsyZr5FcjeCX9p0dq4992vYubyT-iS12SEjVcxrP53Gp5F_VELC3rnRuxUXi\" alt=\"\" width=\"517\" height=\"132\"/></noscript><img class=\" lazyloaded\" loading=\"lazy\" decoding=\"async\" src=\"https://lh3.googleusercontent.com/OtpTAooVApN0aWcsKyPNY2pvJ-GrfR9tavfa0CGj3WyFKfehHvllrRWRi7lAXVDg3j-GXSF-Pn8zrsyZr5FcjeCX9p0dq4992vYubyT-iS12SEjVcxrP53Gp5F_VELC3rnRuxUXi\" data-src=\"https://lh3.googleusercontent.com/OtpTAooVApN0aWcsKyPNY2pvJ-GrfR9tavfa0CGj3WyFKfehHvllrRWRi7lAXVDg3j-GXSF-Pn8zrsyZr5FcjeCX9p0dq4992vYubyT-iS12SEjVcxrP53Gp5F_VELC3rnRuxUXi\" alt=\"\" width=\"517\" height=\"132\"></figure></div>\n",
    "<p>For first reading, new prediction = 40 + ( 0.3 x -16 ) = 35.2</p>\n",
    "<p>The new residual for this reading becomes, 21 – 35.2 = -14</p>\n",
    "<p>Similarly, we continue to do so for all readings. We then use these values for the next model and so on, the loss will keep on optimizing.</p>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><noscript><img loading=\"lazy\" decoding=\"async\" src=\"https://lh4.googleusercontent.com/x6FTWuEGJGs9rcB30YVfpd2uG4uuo_IWhnAkMWvJb_p4HXxKhAylHAoxx6W-KLlK9prBWt20zGbTB_Tf7oma6IEtwO7-P1QXEVgpjNV7uGBTfg4aIbhBiDkzT98kjt5JxgKtMaYS\" alt=\"\" width=\"497\" height=\"176\"/></noscript><img class=\" lazyloaded\" loading=\"lazy\" decoding=\"async\" src=\"https://lh4.googleusercontent.com/x6FTWuEGJGs9rcB30YVfpd2uG4uuo_IWhnAkMWvJb_p4HXxKhAylHAoxx6W-KLlK9prBWt20zGbTB_Tf7oma6IEtwO7-P1QXEVgpjNV7uGBTfg4aIbhBiDkzT98kjt5JxgKtMaYS\" data-src=\"https://lh4.googleusercontent.com/x6FTWuEGJGs9rcB30YVfpd2uG4uuo_IWhnAkMWvJb_p4HXxKhAylHAoxx6W-KLlK9prBWt20zGbTB_Tf7oma6IEtwO7-P1QXEVgpjNV7uGBTfg4aIbhBiDkzT98kjt5JxgKtMaYS\" alt=\"\" width=\"497\" height=\"176\"></figure></div>\n",
    "<h4 class=\"wp-block-heading\"><strong>2. CLASSIFICATION</strong></h4>\n",
    "<p>For classification, let’s try to predict if students pass or fail based on the number of hours studied. For binary classification, the probability for the base model is 0.5, meaning there is a fifty percent chance that a student will either pass or fail. Consequently, the residuals for the first five readings are -0.5, 0.5, -0.5, 0.5 and 0.5, respectively.&nbsp;</p>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><noscript><img loading=\"lazy\" decoding=\"async\" src=\"https://lh3.googleusercontent.com/XxEMliWC385ndmUZ71vzwsDq2-DQMwtwLJLjgMaPO_WpofQNswzC-34QAokbMrvXZCFYfeGg-_0sePocA9AWshKvpjL2UyPYmBL9JhttyNV_ma0tTXlycNz2USEDIpmNaFQX29aj\" alt=\"\" width=\"245\" height=\"199\"/></noscript><img class=\" lazyloaded\" loading=\"lazy\" decoding=\"async\" src=\"https://lh3.googleusercontent.com/XxEMliWC385ndmUZ71vzwsDq2-DQMwtwLJLjgMaPO_WpofQNswzC-34QAokbMrvXZCFYfeGg-_0sePocA9AWshKvpjL2UyPYmBL9JhttyNV_ma0tTXlycNz2USEDIpmNaFQX29aj\" data-src=\"https://lh3.googleusercontent.com/XxEMliWC385ndmUZ71vzwsDq2-DQMwtwLJLjgMaPO_WpofQNswzC-34QAokbMrvXZCFYfeGg-_0sePocA9AWshKvpjL2UyPYmBL9JhttyNV_ma0tTXlycNz2USEDIpmNaFQX29aj\" alt=\"\" width=\"245\" height=\"199\"></figure></div>\n",
    "<p>Similar to regression, we construct the decision tree for Hours greater than 3.5 and then calculate the similarity scores. The difference between the similarity score formula for regression and classification is the loss function. λ remains the same.</p>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><noscript><img loading=\"lazy\" decoding=\"async\" src=\"https://lh6.googleusercontent.com/2xIIjILJavY65Zq5uABPvIBYNVrYQFuyPbZgNv0CPeBPIEkW50MyrcvqCvB2G9XZQ9apRKMD5A8kSEkHVCa5__ER5aKN9ZEMVWXT6ywX2btgQp6DWwYYA58smPSFbfoaOnEBFB3k\" alt=\"\" width=\"304\" height=\"185\"/></noscript><img class=\" lazyloaded\" loading=\"lazy\" decoding=\"async\" src=\"https://lh6.googleusercontent.com/2xIIjILJavY65Zq5uABPvIBYNVrYQFuyPbZgNv0CPeBPIEkW50MyrcvqCvB2G9XZQ9apRKMD5A8kSEkHVCa5__ER5aKN9ZEMVWXT6ywX2btgQp6DWwYYA58smPSFbfoaOnEBFB3k\" data-src=\"https://lh6.googleusercontent.com/2xIIjILJavY65Zq5uABPvIBYNVrYQFuyPbZgNv0CPeBPIEkW50MyrcvqCvB2G9XZQ9apRKMD5A8kSEkHVCa5__ER5aKN9ZEMVWXT6ywX2btgQp6DWwYYA58smPSFbfoaOnEBFB3k\" alt=\"\" width=\"304\" height=\"185\"></figure></div>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><noscript><img loading=\"lazy\" decoding=\"async\" src=\"https://lh3.googleusercontent.com/qDd_MH-Jd6kLbyhcXte3P9jdZB5R4Sjjo5wSSA8jpkVwEO3jeSYEcFTRvvtV2CXqMnbQdoo6gsZb0PnBEJyG9_I1E6tbAyxv8lrJVsxQ6TD582TFgt7yQjd5V0PymnaVRX2L2uBE\" alt=\"\" width=\"381\" height=\"165\"/></noscript><img class=\" lazyloaded\" loading=\"lazy\" decoding=\"async\" src=\"https://lh3.googleusercontent.com/qDd_MH-Jd6kLbyhcXte3P9jdZB5R4Sjjo5wSSA8jpkVwEO3jeSYEcFTRvvtV2CXqMnbQdoo6gsZb0PnBEJyG9_I1E6tbAyxv8lrJVsxQ6TD582TFgt7yQjd5V0PymnaVRX2L2uBE\" data-src=\"https://lh3.googleusercontent.com/qDd_MH-Jd6kLbyhcXte3P9jdZB5R4Sjjo5wSSA8jpkVwEO3jeSYEcFTRvvtV2CXqMnbQdoo6gsZb0PnBEJyG9_I1E6tbAyxv8lrJVsxQ6TD582TFgt7yQjd5V0PymnaVRX2L2uBE\" alt=\"\" width=\"381\" height=\"165\"></figure></div>\n",
    "<p>With λ = 0, the similarity scores for the root and nodes are 0.2, 0.33 and 2, respectively. The gain is 2.13 which is calculated similarly as we calculate for regression part.</p>\n",
    "<p>Pruning in classification is complete differently than in regression. In classification, we calculate the cover value of each branch. If the gain is greater than the cover value only then further splitting is ready else the branch is cut. In this case, both branches can split. This is how pruning is done in XGboost classification.</p>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><noscript><img loading=\"lazy\" decoding=\"async\" src=\"https://lh6.googleusercontent.com/XR4MNkCIfo0_xoPvF7-5MyVJtxeqnwPuanU0clreA3XfDpYdCDOny-rqdTd4KZ6St48CeuZEuJMBlLAP6412riQGLY9tXdSPSx1b1vwPxjs2IiOwLKoQrwo48Im-YgkMkyUL0b77\" alt=\"\" width=\"307\" height=\"56\"/></noscript><img class=\" lazyloaded\" loading=\"lazy\" decoding=\"async\" src=\"https://lh6.googleusercontent.com/XR4MNkCIfo0_xoPvF7-5MyVJtxeqnwPuanU0clreA3XfDpYdCDOny-rqdTd4KZ6St48CeuZEuJMBlLAP6412riQGLY9tXdSPSx1b1vwPxjs2IiOwLKoQrwo48Im-YgkMkyUL0b77\" data-src=\"https://lh6.googleusercontent.com/XR4MNkCIfo0_xoPvF7-5MyVJtxeqnwPuanU0clreA3XfDpYdCDOny-rqdTd4KZ6St48CeuZEuJMBlLAP6412riQGLY9tXdSPSx1b1vwPxjs2IiOwLKoQrwo48Im-YgkMkyUL0b77\" alt=\"\" width=\"307\" height=\"56\"></figure></div>\n",
    "<p>For the new predictions, we first find the new probability using the log(odds) function. Using the log(odds) formula, the log(odds) value for the base model is 0 (p=0.5).</p>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><noscript><img loading=\"lazy\" decoding=\"async\" src=\"https://lh5.googleusercontent.com/K2hL4JOxs1tl_yL5vBvXPxeu52-Q5Aa4bZU6fNNLFiqAIQ2ALRCee3a8X-EW2AUUSU0YNcfnrDEHGx8cBpPk0uDzIp3uo0_U9QikeAXO7oUQHUmvQIbhxnXHjZlVtIUl5f43XIgV\" alt=\"\" width=\"279\" height=\"93\"/></noscript><img class=\" lazyloaded\" loading=\"lazy\" decoding=\"async\" src=\"https://lh5.googleusercontent.com/K2hL4JOxs1tl_yL5vBvXPxeu52-Q5Aa4bZU6fNNLFiqAIQ2ALRCee3a8X-EW2AUUSU0YNcfnrDEHGx8cBpPk0uDzIp3uo0_U9QikeAXO7oUQHUmvQIbhxnXHjZlVtIUl5f43XIgV\" data-src=\"https://lh5.googleusercontent.com/K2hL4JOxs1tl_yL5vBvXPxeu52-Q5Aa4bZU6fNNLFiqAIQ2ALRCee3a8X-EW2AUUSU0YNcfnrDEHGx8cBpPk0uDzIp3uo0_U9QikeAXO7oUQHUmvQIbhxnXHjZlVtIUl5f43XIgV\" alt=\"\" width=\"279\" height=\"93\"></figure></div>\n",
    "<h4 class=\"wp-block-heading\">For the first reading, we calculate the log(odds) prediction value as..</h4>\n",
    "<p>0 + (0.3 x -0.66) = -0.198</p>\n",
    "<p>Where 0.3 is the learning rate(eta) like in XGboost regression and -0.66 is the output value is..</p>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><noscript><img loading=\"lazy\" decoding=\"async\" src=\"https://lh4.googleusercontent.com/aRV8Kc4zjNPNChL4ca9TQdGGXjS78J7OXeLU9yp5b43u031gL8BKzL3CMcmD5-8CtpFeHQZXe_GmhCZFr_kzW5lIggcrzA-4DxpOumkFU_5E-dykvdY5Gdzbu2GU_UHBMIUxcrVW\" alt=\"\" width=\"326\" height=\"87\"/></noscript><img class=\" lazyloaded\" loading=\"lazy\" decoding=\"async\" src=\"https://lh4.googleusercontent.com/aRV8Kc4zjNPNChL4ca9TQdGGXjS78J7OXeLU9yp5b43u031gL8BKzL3CMcmD5-8CtpFeHQZXe_GmhCZFr_kzW5lIggcrzA-4DxpOumkFU_5E-dykvdY5Gdzbu2GU_UHBMIUxcrVW\" data-src=\"https://lh4.googleusercontent.com/aRV8Kc4zjNPNChL4ca9TQdGGXjS78J7OXeLU9yp5b43u031gL8BKzL3CMcmD5-8CtpFeHQZXe_GmhCZFr_kzW5lIggcrzA-4DxpOumkFU_5E-dykvdY5Gdzbu2GU_UHBMIUxcrVW\" alt=\"\" width=\"326\" height=\"87\"></figure></div>\n",
    "<p>We then need to convert this log(odds) value into probability. For this, we use the logistic function.</p>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><noscript><img loading=\"lazy\" decoding=\"async\" src=\"https://lh6.googleusercontent.com/T3e_TojnhL881GmCVSrTEIwKbuZQEBEdXRQhClGN-6UvcySH6a2yNX3VjNWAXrMUbewTBBraGudPzJEuit51FXro_aeR_ZPMFY8vapL_-PRMMtMs8e8qYE9dhcFDZriU9gg99DVL\" alt=\"\" width=\"278\" height=\"90\"/></noscript><img class=\" lazyloaded\" loading=\"lazy\" decoding=\"async\" src=\"https://lh6.googleusercontent.com/T3e_TojnhL881GmCVSrTEIwKbuZQEBEdXRQhClGN-6UvcySH6a2yNX3VjNWAXrMUbewTBBraGudPzJEuit51FXro_aeR_ZPMFY8vapL_-PRMMtMs8e8qYE9dhcFDZriU9gg99DVL\" data-src=\"https://lh6.googleusercontent.com/T3e_TojnhL881GmCVSrTEIwKbuZQEBEdXRQhClGN-6UvcySH6a2yNX3VjNWAXrMUbewTBBraGudPzJEuit51FXro_aeR_ZPMFY8vapL_-PRMMtMs8e8qYE9dhcFDZriU9gg99DVL\" alt=\"\" width=\"278\" height=\"90\"></figure></div>\n",
    "<p>Plugging in log(odds) = -0.198 in the above formula, we get a probability of ≈0.45.</p>\n",
    "<p>The new residual for this reading becomes -0.45. Just like in regression we continue the process to optimize the loss until the residuals become very small or have reached the maximum number.</p>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><noscript><img loading=\"lazy\" decoding=\"async\" src=\"https://lh4.googleusercontent.com/8o-e22n65nJgyQrRK36HEIRKsw8ibMNBVRk2S6UlNJhyApwK7xibl6PlCLqNVa9IlNDROSBk6k3TCb4FHvEBPXxSFUtGRg8IC8DxSoezmsQz5IcrMG2R5pV8AIzQltjiwT0Rm-z7\" alt=\"\" width=\"416\" height=\"168\"/></noscript><img class=\" lazyloaded\" loading=\"lazy\" decoding=\"async\" src=\"https://lh4.googleusercontent.com/8o-e22n65nJgyQrRK36HEIRKsw8ibMNBVRk2S6UlNJhyApwK7xibl6PlCLqNVa9IlNDROSBk6k3TCb4FHvEBPXxSFUtGRg8IC8DxSoezmsQz5IcrMG2R5pV8AIzQltjiwT0Rm-z7\" data-src=\"https://lh4.googleusercontent.com/8o-e22n65nJgyQrRK36HEIRKsw8ibMNBVRk2S6UlNJhyApwK7xibl6PlCLqNVa9IlNDROSBk6k3TCb4FHvEBPXxSFUtGRg8IC8DxSoezmsQz5IcrMG2R5pV8AIzQltjiwT0Rm-z7\" alt=\"\" width=\"416\" height=\"168\"></figure></div>\n",
    "<p>For both XGboost classification and regression, as the value of λ increases, more pruning takes place due to a decrease in the similarity score and smaller output values. An increase in the value of the regularization parameter reduces the sensitivity of the model towards individual observations. As a result, the regularization parameter takes care of outliers to an extent and reduces overfitting.</p>\n",
    "<h3 class=\"wp-block-heading\"><strong>PYTHON IMPLEMENTATION</strong></h3>\n",
    "<p>Let’s take a look at the implementation of XGBClassifier() in python.</p>\n",
    "<ol><li><strong>Import Libraries</strong></li></ol>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><noscript><img loading=\"lazy\" decoding=\"async\" src=\"https://lh4.googleusercontent.com/fxnvdqtH6m4KhPl7r_0dr2an-BqbPbK7-G2dLR22hk1f8jz9N3VhQG_R9XIaq_SXUcKKAYsOtZ8WnJZ3uqC_Za0Kq8bq-7IISmw-YrGopycMV-9IBuqZDUcA9WNZgOEI50v_QR6m\" alt=\"\" width=\"589\" height=\"185\"/></noscript><img class=\" lazyloaded\" loading=\"lazy\" decoding=\"async\" src=\"https://lh4.googleusercontent.com/fxnvdqtH6m4KhPl7r_0dr2an-BqbPbK7-G2dLR22hk1f8jz9N3VhQG_R9XIaq_SXUcKKAYsOtZ8WnJZ3uqC_Za0Kq8bq-7IISmw-YrGopycMV-9IBuqZDUcA9WNZgOEI50v_QR6m\" data-src=\"https://lh4.googleusercontent.com/fxnvdqtH6m4KhPl7r_0dr2an-BqbPbK7-G2dLR22hk1f8jz9N3VhQG_R9XIaq_SXUcKKAYsOtZ8WnJZ3uqC_Za0Kq8bq-7IISmw-YrGopycMV-9IBuqZDUcA9WNZgOEI50v_QR6m\" alt=\"\" width=\"589\" height=\"185\"></figure></div>\n",
    "<ol start=\"2\"><li><strong>Load Data</strong></li></ol>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><noscript><img loading=\"lazy\" decoding=\"async\" src=\"https://lh4.googleusercontent.com/wKLvp3ITNWGLdrnhEyINFABN6EPNr-JpmIVXBRrr--jziYyw3-CZCMTgexipCiOMTBXpP4nxHuHfO9pLMt9zz4AqynAzH_VnmyEgd4b_EXceQW42Yxur8CMOyzVW434kPu57TlbA\" alt=\"\" width=\"673\" height=\"180\"/></noscript><img class=\" lazyloaded\" loading=\"lazy\" decoding=\"async\" src=\"https://lh4.googleusercontent.com/wKLvp3ITNWGLdrnhEyINFABN6EPNr-JpmIVXBRrr--jziYyw3-CZCMTgexipCiOMTBXpP4nxHuHfO9pLMt9zz4AqynAzH_VnmyEgd4b_EXceQW42Yxur8CMOyzVW434kPu57TlbA\" data-src=\"https://lh4.googleusercontent.com/wKLvp3ITNWGLdrnhEyINFABN6EPNr-JpmIVXBRrr--jziYyw3-CZCMTgexipCiOMTBXpP4nxHuHfO9pLMt9zz4AqynAzH_VnmyEgd4b_EXceQW42Yxur8CMOyzVW434kPu57TlbA\" alt=\"\" width=\"673\" height=\"180\"></figure></div>\n",
    "<ol start=\"3\"><li><strong>Train-Test Split</strong></li></ol>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><noscript><img loading=\"lazy\" decoding=\"async\" src=\"https://lh5.googleusercontent.com/NI_kf7yTPG7E_UVuqYCvjFIl0CU3Olp088gCTe6L3PGs8AsUMF9h7Z_B_Xxw6lxZNueN3Cx0x78iFF8iyQGL9zUZG-jLiknf7vQBmPu1R-6y91Jj9xSp6Wpoi7oOGP0Hz9ptHsrf\" alt=\"\" width=\"688\" height=\"72\"/></noscript><img class=\" lazyloaded\" loading=\"lazy\" decoding=\"async\" src=\"https://lh5.googleusercontent.com/NI_kf7yTPG7E_UVuqYCvjFIl0CU3Olp088gCTe6L3PGs8AsUMF9h7Z_B_Xxw6lxZNueN3Cx0x78iFF8iyQGL9zUZG-jLiknf7vQBmPu1R-6y91Jj9xSp6Wpoi7oOGP0Hz9ptHsrf\" data-src=\"https://lh5.googleusercontent.com/NI_kf7yTPG7E_UVuqYCvjFIl0CU3Olp088gCTe6L3PGs8AsUMF9h7Z_B_Xxw6lxZNueN3Cx0x78iFF8iyQGL9zUZG-jLiknf7vQBmPu1R-6y91Jj9xSp6Wpoi7oOGP0Hz9ptHsrf\" alt=\"\" width=\"688\" height=\"72\"></figure></div>\n",
    "<ol start=\"4\"><li><strong>Train model</strong></li></ol>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><noscript><img loading=\"lazy\" decoding=\"async\" src=\"https://lh6.googleusercontent.com/-hzr-Xxd5MoZssi6L4omZGHti_wW2yLJSGdlTxbWocOdREp4ubZaORGWhHeRtipFWYSES4fwqRKnZSFawbPb76u0FfM5hQG3tPlsRo4IoGEnVpr42JhlfJkxhTEpASyU6tEA2Vd9\" alt=\"\" width=\"618\" height=\"217\"/></noscript><img class=\" lazyloaded\" loading=\"lazy\" decoding=\"async\" src=\"https://lh6.googleusercontent.com/-hzr-Xxd5MoZssi6L4omZGHti_wW2yLJSGdlTxbWocOdREp4ubZaORGWhHeRtipFWYSES4fwqRKnZSFawbPb76u0FfM5hQG3tPlsRo4IoGEnVpr42JhlfJkxhTEpASyU6tEA2Vd9\" data-src=\"https://lh6.googleusercontent.com/-hzr-Xxd5MoZssi6L4omZGHti_wW2yLJSGdlTxbWocOdREp4ubZaORGWhHeRtipFWYSES4fwqRKnZSFawbPb76u0FfM5hQG3tPlsRo4IoGEnVpr42JhlfJkxhTEpASyU6tEA2Vd9\" alt=\"\" width=\"618\" height=\"217\"></figure></div>\n",
    "<ol start=\"5\"><li><strong>Cross-Validation</strong></li></ol>\n",
    "<div class=\"wp-block-image\"><figure class=\"aligncenter is-resized\"><noscript><img loading=\"lazy\" decoding=\"async\" src=\"https://lh3.googleusercontent.com/oLwAJ5ysitbqSvDJc9EZAGC0ZG8dSdJaoQdRNOmwGYfNESYRfYc8N33MQY3xnbbPszbltMChkOfRlFW8_RbElhOgiRBrq9ugU6ZSsLtCFzOIFS9WSVF6Yp2HgAiHUVA9HhHYM80S\" alt=\"\" width=\"469\" height=\"115\"/></noscript><img class=\" lazyloaded\" loading=\"lazy\" decoding=\"async\" src=\"https://lh3.googleusercontent.com/oLwAJ5ysitbqSvDJc9EZAGC0ZG8dSdJaoQdRNOmwGYfNESYRfYc8N33MQY3xnbbPszbltMChkOfRlFW8_RbElhOgiRBrq9ugU6ZSsLtCFzOIFS9WSVF6Yp2HgAiHUVA9HhHYM80S\" data-src=\"https://lh3.googleusercontent.com/oLwAJ5ysitbqSvDJc9EZAGC0ZG8dSdJaoQdRNOmwGYfNESYRfYc8N33MQY3xnbbPszbltMChkOfRlFW8_RbElhOgiRBrq9ugU6ZSsLtCFzOIFS9WSVF6Yp2HgAiHUVA9HhHYM80S\" alt=\"\" width=\"469\" height=\"115\"></figure></div>\n",
    "<ol start=\"6\"><li><strong>Prediction</strong></li></ol>\n",
    "<div class=\"wp-block-image\"><img decoding=\"async\" src=\"https://lh5.googleusercontent.com/K9ufiRozlC5c42MXL69TNQen9GZx5YZzIHBk_6E_LU5bB4kJ6Ifhp26uoNyLyNGtp45rgALq3wp1abz4x9IzDgutFg-45_HlYGJO637j6GUJS_plFhUlrdhW_MI8Tq-FGT7IA2uq\" alt=\"\" width=\"297\" height=\"140\"/><img decoding=\"async\" src=\"data:image/svg+xml,%3Csvg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20297%20140%22%3E%3C/svg%3E\" data-src=\"https://lh5.googleusercontent.com/K9ufiRozlC5c42MXL69TNQen9GZx5YZzIHBk_6E_LU5bB4kJ6Ifhp26uoNyLyNGtp45rgALq3wp1abz4x9IzDgutFg-45_HlYGJO637j6GUJS_plFhUlrdhW_MI8Tq-FGT7IA2uq\" alt=\"\" width=\"297\" height=\"140\"></div>\n",
    "<p>Our model in this case performs extremely well to give 100% accuracy. However, this will not always be the case, and we will need to work around it accordingly.</p>\n",
    "<h3 class=\"wp-block-heading\"><strong>CONCLUSION</strong></h3>\n",
    "<p>XGBoost is one of the most popular high-performance algorithms out there. Hopefully, this article gives you a brief introduction to XGBoost in machine learning, it’s functioning and execution, giving you a foundation to explore further. Try using it in projects and competitions, XGBoost might just end up putting you on top of the leaderboard.&nbsp;</p>\n",
    "</div><!-- .entry-content .clear -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"wp-block-heading\" id=\"h-xgboost-vs-gradient-boosting\">XGBoost vs Gradient Boosting</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table table-bordered border-black table-striped\"><thead><tr><th>Feature</th><th>XGBoost</th><th>Gradient Boosting</th></tr></thead><tbody><tr><td>Description</td><td>Advanced implementation of gradient boosting</td><td>Ensemble technique using weak learners</td></tr><tr><td>Optimization</td><td>Regularized objective function</td><td>Error gradient minimization</td></tr><tr><td>Efficiency</td><td>Highly optimized, efficient</td><td>Computationally intensive</td></tr><tr><td>Missing Values</td><td>Built-in support</td><td>Requires preprocessing</td></tr><tr><td>Regularization</td><td>Built-in L1 and L2</td><td>Requires external steps</td></tr><tr><td>Feature Importance</td><td>Built-in measures</td><td>Limited, needs external calculation</td></tr><tr><td>Interpretability</td><td>Complex, less interpretable</td><td>More interpretable models</td></tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"wp-block-heading\" id=\"h-difference-between-xgboost-and-random-forest\">Difference between XGBoost and Random Forest</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table table-bordered border-black table-striped\"><thead><tr><th>Feature</th><th>XGBoost</th><th>Random Forest</th></tr></thead><tbody><tr><td>Description</td><td>Improves mistakes from previous trees</td><td>Builds trees independently</td></tr><tr><td>Algorithm Type</td><td>Boosting</td><td>Bagging</td></tr><tr><td>Handling of Weak Learners</td><td>Corrects errors sequentially</td><td>Combines predictions of independently built trees</td></tr><tr><td>Regularization</td><td>Uses L1 and L2 regularization to prevent overfitting</td><td>Usually doesn’t employ regularization techniques</td></tr><tr><td>Performance</td><td>Often performs better on structured data but needs more tuning</td><td>Simpler and less prone to overfitting</td></tr></tbody></table>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
