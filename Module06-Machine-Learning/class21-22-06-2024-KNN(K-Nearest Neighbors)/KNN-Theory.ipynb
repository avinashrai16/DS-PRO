{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a simple, yet powerful, supervised machine learning algorithm used for both classification and regression tasks. It operates on the principle that similar instances exist in close proximity to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Algorithm can be used for both classification and regression predictive problems. However, it is more widely used in classification problems in the industry. To evaluate any technique, we generally look at 3 important aspects:\n",
    "\n",
    "1. Ease of interpreting output\n",
    "\n",
    "2. Calculation time\n",
    "\n",
    "3. Predictive Power\n",
    "\n",
    "Let us take a few examples to  place KNN in the scale :\n",
    "\n",
    "<img src=\"Model-comparison32221.png\" width=\"450\">\n",
    "\n",
    "\n",
    "KNN classifier fairs across all parameters of consideration. It is commonly used for its ease of interpretation and low calculation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does the KNN Algorithm Work?\n",
    "Let’s take a simple case to understand this algorithm. Following is a spread of red circles (RC) and green squares (GS):\n",
    "\n",
    "<img src=\"image-20.webp\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You intend to find out the class of the blue star (BS). BS can either be RC or GS and nothing else. The “K” in KNN algorithm is the nearest neighbor we wish to take the vote from. Let’s say K = 3. Hence, we will now make a circle with BS as the center just as big as to enclose only three data points on the plane. Refer to the following diagram for more details:\n",
    "\n",
    "<img src=\"scenario2.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three closest points to BS are all RC. Hence, with a good confidence level, we can say that the BS should belong to the class RC. Here, the choice became obvious as all three votes from the closest neighbor went to RC. The choice of the parameter K is very crucial in this algorithm. Next, we will understand the factors to be considered to conclude the best K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts of KNN:\n",
    "1. ***Instance-Based Learning:*** KNN is a type of instance-based learning where the model makes predictions based on the entire training dataset. Unlike other algorithms, it doesn’t explicitly learn a model from the training data.\n",
    "\n",
    "2. ***Lazy Learning:*** KNN is considered a lazy learner because it doesn't build a model until a prediction is requested. It simply stores the training dataset and performs computations at the time of prediction.\n",
    "\n",
    "3. ***Distance Metric:*** The core idea behind KNN is to find the 'k' nearest neighbors to a given query point. The distance between instances is typically measured using metrics like Euclidean distance, Manhattan distance, or Minkowski distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How KNN Works:\n",
    "\n",
    "1. Choose the number of 'K' neighbors: Decide the number of neighbors, 'k', to consider for making the prediction. This is a crucial hyperparameter that can significantly impact the model's performance.\n",
    "\n",
    "2. Compute Distance: Calculate the distance between the query point and all the points in the training dataset.\n",
    "\n",
    "3. Identify Nearest Neighbors: Select the 'k' points that are closest to the query point.\n",
    "\n",
    "4. Predict Output:\n",
    "\n",
    "* ***For classification:*** The class label of the query point is determined by the majority class among the 'k' nearest neighbors.\n",
    "* ***For regression:*** The predicted value is the average (or weighted average) of the values of the 'k' nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of KNN: \n",
    "\n",
    "* Simplicity: KNN is easy to understand and implement.\n",
    "* No Training Phase: Since it’s a lazy learner, there’s no explicit training phase, making it suitable for scenarios where the training data is frequently updated.\n",
    "* Versatility: Can be used for both classification and regression problems.\n",
    "\n",
    "### Disadvantages of KNN:\n",
    "\n",
    "* Computational Cost: KNN can be computationally expensive, especially with large datasets, because it requires calculating the distance between the query point and all points in the training set.\n",
    "* Storage Requirements: Since KNN stores all training data, it can require significant storage space.\n",
    "* Sensitivity to Irrelevant Features: KNN’s performance can degrade if the dataset contains irrelevant or redundant features. Feature scaling and selection are important pre-processing steps.\n",
    "* Curse of Dimensionality: The algorithm can struggle in high-dimensional spaces, where the distance between points becomes less meaningful.\n",
    "\n",
    "### Applications of KNN:\n",
    "* Recommendation Systems: Used in collaborative filtering to recommend products based on user similarity.\n",
    "* Image Recognition: KNN can classify images based on similarity to other images.\n",
    "* Medical Diagnosis: Assists in diagnosing diseases by finding patients with similar symptoms and medical history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------- --------------------\n",
    "\n",
    "## Flow of KNN Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"knn-algorithm.png\" width=\"300\">\n",
    "\n",
    "<img src=\"euclidean.png\" width=\"470\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Flowchart-of-KNN-Method.ppm\" width=\"363\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"manhattan_distance.jpg\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"manhattan_euclidean-distance.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------\n",
    "### KNN Regressor\n",
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of new data point will be avg. of nearest K data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how to select the optimal K value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are no pre-defined statistical methods to find the most favorable value of K.\n",
    "* Initialize a random K value and start computing.\n",
    "* Choosing a small value of K leads to unstable decision boundaries.\n",
    "* The substantial K value is better for classification as it leads to smoothening the decision boundaries.\n",
    "* Derive a plot between error rate and K denoting values in a defined range. Then choose the K value as having a minimum error rate.\n",
    "* The small K value isn’t suitable for classification.\n",
    "* ***The optimal K value usually found is the square root of N, where N is the total number of samples.***\n",
    "* Use an error plot or accuracy plot to find the most favorable K value.\n",
    "* KNN performs well with multi-label classes, but you must be aware of the outliers.\n",
    "* KNN is used broadly in the area of pattern recognition and analytical evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants of KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***KD-Tree KNN***\n",
    "\n",
    "KD-Tree is a data structure that partitions the space to organize points in a k-dimensional space. This variant of KNN uses a KD-Tree to efficiently query the nearest neighbors, significantly reducing the computational cost for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A kd — Tree (k-dimensional tree) is a data structure that recursively subdivides the space into regions associated with specific data points. The primary objective of a kd — Tree is to facilitate efficient multidimensional search operations, particularly nearest-neighbor searches. The algorithm constructs a binary tree in which each node represents a region in the multidimensional space, and the associated hyperplane is aligned with one of the coordinate axes. At each level of the tree, the algorithm selects a dimension to split the data, creating two child nodes. This process continues recursively until a termination condition is met, such as a predefined depth or a threshold number of points per node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference how KD Tree works\n",
    "https://www.analyticsvidhya.com/blog/2017/11/information-retrieval-using-kdtree/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ball Tree KNN***\n",
    "\n",
    "Similar to KD-Tree, Ball Tree is another data structure for organizing points in a metric space. It is more efficient than KD-Tree for high-dimensional data. Ball Tree KNN uses this structure to quickly find nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
