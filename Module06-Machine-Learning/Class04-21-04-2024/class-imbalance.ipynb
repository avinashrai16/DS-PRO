{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "    class-imbalace\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class imbalance refers to a situation in a classification problem where the distribution of classes is uneven, meaning one class (the minority class) is significantly less represented than the other class or classes (the majority class or classes). This issue commonly occurs in various fields such as medical diagnosis, fraud detection, anomaly detection, and natural language processing.\n",
    "\n",
    "The challenge with class imbalance is that it can lead to biased models that are overly influenced by the majority class, resulting in poor performance on the minority class. Models trained on imbalanced data may exhibit high accuracy on the majority class but poor accuracy on the minority class, which is often the class of interest.\n",
    "\n",
    "Several techniques can be employed to address class imbalance, including:\n",
    "\n",
    "1. **Resampling methods**: These involve either oversampling the minority class (creating synthetic samples) or undersampling the majority class (removing samples). Examples include Random Oversampling, SMOTE (Synthetic Minority Over-sampling Technique), and NearMiss.\n",
    "\n",
    "2. **Algorithmic approaches**: Some machine learning algorithms are inherently capable of handling class imbalance better than others. For example, ensemble methods like Random Forests and gradient boosting are often more robust to class imbalance.\n",
    "\n",
    "3. **Cost-sensitive learning**: This involves assigning different costs to misclassification errors for different classes, penalizing misclassification of the minority class more heavily.\n",
    "\n",
    "4. **Anomaly detection techniques**: In situations where the minority class represents anomalies or rare events, anomaly detection methods can be used instead of traditional classification techniques.\n",
    "\n",
    "5. **Algorithmic adjustments**: Certain algorithms have parameters that can be tuned to adjust for class imbalance, such as setting class weights in classifiers like SVM (Support Vector Machine) or logistic regression.\n",
    "\n",
    "6. **Evaluation metrics**: Instead of solely relying on accuracy, metrics like precision, recall, F1-score, ROC-AUC (Receiver Operating Characteristic - Area Under the Curve), and PR-AUC (Precision-Recall Area Under the Curve) provide a more nuanced understanding of model performance in the presence of class imbalance.\n",
    "\n",
    "Addressing class imbalance requires careful consideration of the specific problem and dataset characteristics, as well as experimentation with different techniques to find the most effective approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"exampleofclassimbalance.webp\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UnderSampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling involves randomly removing instances from the majority class to balance class distribution. It's a simple but effective technique to mitigate class imbalance in machine learning. However, it may lead to loss of information from the majority class and potential underfitting if not managed carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OverSampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling is a technique used to address class imbalance by artificially increasing the number of instances in the minority class. It involves replicating existing instances or generating synthetic data points for the minority class. While it helps balance class distribution, it may lead to overfitting if not properly controlled and can be computationally expensive for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"over-under-sampling.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "    SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE (Synthetic Minority Over-sampling Technique) is a method used to address class imbalance **by generating synthetic samples for the minority class.** It works by creating synthetic instances along the line segments joining any/all of the k minority class nearest neighbors. SMOTE helps to balance class distribution and improve model performance, particularly in situations where the minority class is underrepresented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"smote.jpeg\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, here's a comparison of **SMOTE, oversampling, and undersampling** in tabular format:\n",
    "\n",
    "| Technique       | Description                                                                                                         | Advantages                                                                                     | Disadvantages                                                                                                      |\n",
    "|-----------------|---------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|\n",
    "| SMOTE           | Generates synthetic instances for the minority class by interpolating between existing instances.                   | - Addresses class imbalance effectively. <br>- Creates diverse synthetic samples.               | - May introduce noise if synthetic samples are generated excessively.                                             |\n",
    "| Oversampling    | Increases the number of instances in the minority class by replicating existing instances or generating new ones.   | - Simple to implement. <br>- Can be effective for small to moderately imbalanced datasets.       | - May lead to overfitting if not controlled. <br>- Can be computationally expensive for large datasets.            |\n",
    "| Undersampling   | Reduces the number of instances in the majority class by randomly selecting a subset of instances.                  | - Simple and computationally efficient. <br>- Can help to mitigate class imbalance.             | - May discard valuable information from the majority class, potentially leading to underfitting.                    |\n",
    "\n",
    "Each technique has its advantages and disadvantages, and the choice depends on factors such as the dataset size, class distribution, computational resources, and the specific requirements of the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Industry use case**\n",
    "\n",
    "we can combine undersampling  with SMOTE to create more balanced data set and build a model on top of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
